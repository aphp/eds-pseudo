{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73fd701e-94b1-4d7c-977f-26bedec37821",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cff0ee7-2656-4f79-b5d3-4361eb95c7ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import numpy as np\n",
    "import spacy\n",
    "import itertools\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from collections.abc import Sized\n",
    "from itertools import chain, repeat\n",
    "from pathlib import Path\n",
    "from typing import (\n",
    "    TYPE_CHECKING,\n",
    "    List,\n",
    "    Optional,\n",
    "    Tuple,\n",
    ")\n",
    "\n",
    "import spacy\n",
    "import torch\n",
    "from accelerate import Accelerator\n",
    "from confit import Config\n",
    "from confit import Cli\n",
    "from confit.utils.random import set_seed\n",
    "from rich_logger import RichTablePrinter\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from eds_pseudo.adapter import PseudoReader\n",
    "from eds_pseudo.scorer import PseudoScorer\n",
    "from edsnlp.core.pipeline import Pipeline\n",
    "from edsnlp.core.registries import registry\n",
    "from edsnlp.optimization import LinearSchedule, ScheduledOptimizer\n",
    "from edsnlp.pipes.trainable.embeddings.transformer.transformer import Transformer\n",
    "from edsnlp.utils.collections import batchify\n",
    "from edsnlp.utils.typing import AsList\n",
    "\n",
    "BASE_DIR = Path.cwd()\n",
    "\n",
    "from src.eds_pseudo.scripts.train import BatchSizeArg, LengthSortedBatchSampler, SubBatchCollater\n",
    "import edsnlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c20f80c-77aa-4cae-b9fc-1a931048cca2",
   "metadata": {},
   "source": [
    "\n",
    "Le but de ce notebook est de montrer comment passer du fichier train.py et de la config de edsnlp a un notebook qui permet de calculer le premier loss sur le premier batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51752cf-ae5c-42c9-90e7-d401abafae87",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# 1 - Chargement des configs et du pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54d8bb66-6af0-4428-afe5-3afc4a9bcf22",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vars': {'ml_spans': 'pseudo-ml',\n",
       "  'rb_spans': 'pseudo-rb',\n",
       "  'hybrid_spans': 'ents',\n",
       "  'limit': -1,\n",
       "  'labels': ['ADRESSE',\n",
       "   'DATE',\n",
       "   'DATE_NAISSANCE',\n",
       "   'IPP',\n",
       "   'MAIL',\n",
       "   'NDA',\n",
       "   'NOM',\n",
       "   'PRENOM',\n",
       "   'SECU',\n",
       "   'TEL',\n",
       "   'VILLE',\n",
       "   'ZIP']},\n",
       " 'nlp': {'@core': 'pipeline',\n",
       "  'lang': 'eds',\n",
       "  'pipeline': ['normalizer',\n",
       "   'simple-rules',\n",
       "   'addresses',\n",
       "   'context',\n",
       "   'ner',\n",
       "   'clean',\n",
       "   'merge',\n",
       "   'dates-normalizer'],\n",
       "  'batch_size': 32,\n",
       "  'components': ${components}},\n",
       " 'components': {'normalizer': {'@factory': 'eds.normalizer'},\n",
       "  'sentencizer': {'@factory': 'eds.sentences'},\n",
       "  'remove-lowercase': {'@factory': 'eds.remove_lowercase'},\n",
       "  'dates': {'@factory': 'eds_pseudo.dates', 'span_setter': ${vars.rb_spans}},\n",
       "  'simple-rules': {'@factory': 'eds_pseudo.simple_rules',\n",
       "   'pattern_keys': ['TEL', 'MAIL', 'SECU'],\n",
       "   'span_setter': ${vars.rb_spans}},\n",
       "  'addresses': {'@factory': 'eds_pseudo.addresses',\n",
       "   'span_setter': ${vars.rb_spans}},\n",
       "  'context': {'@factory': 'eds_pseudo.context',\n",
       "   'span_setter': ${vars.rb_spans}},\n",
       "  'embedding': {'@factory': 'eds.text_cnn',\n",
       "   'kernel_sizes': [3],\n",
       "   'embedding': {'@factory': 'eds.transformer',\n",
       "    'model': '/export/home/amessager/projet/pseudo_champs_courts/data/eds-camembert-pseudo',\n",
       "    'window': 128,\n",
       "    'stride': 96,\n",
       "    'new_tokens': [['(?:\\\\n\\\\s*)*\\\\n', '⏎']]}},\n",
       "  'ner': {'@factory': 'eds.ner_crf',\n",
       "   'mode': 'joint',\n",
       "   'target_span_getter': ${vars.ml_spans},\n",
       "   'span_setter': ${vars.ml_spans},\n",
       "   'embedding': ${components.embedding}},\n",
       "  'clean': {'@factory': 'eds_pseudo.clean',\n",
       "   'span_getter': [${vars.rb_spans}, ${vars.ml_spans}, ${vars.hybrid_spans}]},\n",
       "  'merge': {'@factory': 'eds_pseudo.merge',\n",
       "   'span_getter': [${vars.rb_spans}, ${vars.ml_spans}],\n",
       "   'span_setter': ['ents', ${vars.hybrid_spans}, '*']},\n",
       "  'dates-normalizer': {'@factory': 'eds_pseudo.dates_normalizer',\n",
       "   'span_getter': {'ents': ['DATE', 'DATE_NAISSANCE']},\n",
       "   'format': 'strftime'}},\n",
       " 'scorers': {'rb_spans': ${vars.rb_spans},\n",
       "  'ml_spans': ${vars.ml_spans},\n",
       "  'hybrid_spans': ${vars.hybrid_spans},\n",
       "  'labels': ${vars.labels}},\n",
       " 'training_docs': {'randomize': True,\n",
       "  'max_length': 256,\n",
       "  'multi_sentence': True,\n",
       "  'limit': ${vars.limit},\n",
       "  'source': {'@readers': 'json',\n",
       "   'converter': 'pseudo',\n",
       "   'path': '/export/home/amessager/projet/pseudo_champs_courts/data/training/private_small_train_data.jsonl',\n",
       "   'doc_attributes': ['context', 'note_datetime', 'note_class_source_value'],\n",
       "   'span_setter': [${vars.ml_spans}, ${vars.rb_spans}, ${vars.hybrid_spans}]}},\n",
       " 'val_docs': {'source': {'@readers': 'json',\n",
       "   'converter': 'pseudo',\n",
       "   'path': '/export/home/amessager/projet/pseudo_champs_courts/data/training/private_small_dev_data.jsonl',\n",
       "   'doc_attributes': ['context', 'note_datetime', 'note_class_source_value'],\n",
       "   'span_setter': [${vars.ml_spans}, ${vars.rb_spans}, ${vars.hybrid_spans}]}},\n",
       " 'test_docs': {'source': {'@readers': 'json',\n",
       "   'converter': 'pseudo',\n",
       "   'path': '/export/home/amessager/projet/pseudo_champs_courts/data/training/private_small_test_data.jsonl',\n",
       "   'doc_attributes': ['context', 'note_datetime', 'note_class_source_value'],\n",
       "   'span_setter': [${vars.ml_spans}, ${vars.rb_spans}, ${vars.hybrid_spans}]}},\n",
       " 'train': {'nlp': ${nlp},\n",
       "  'max_steps': 10,\n",
       "  'validation_interval': ${train.max_steps//10},\n",
       "  'batch_size': '2000 words',\n",
       "  'embedding_lr': 5e-05,\n",
       "  'task_lr': 5e-05,\n",
       "  'seed': 43,\n",
       "  'scorer': ${scorers},\n",
       "  'train_data': [${training_docs}],\n",
       "  'val_data': ${val_docs},\n",
       "  'grad_accumulation_max_tokens': 32000,\n",
       "  'output_dir': '/export/home/amessager/projet/pseudo_champs_courts/data/training/artifacts/'},\n",
       " 'evaluate': {'scorer': ${scorers},\n",
       "  'dataset_name': 'AP-HP Pseudo Test',\n",
       "  'data': ${test_docs}},\n",
       " 'package': {'name': 'eds-pseudo-public',\n",
       "  'hf_name': 'AP-HP/eds-pseudo-public'}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_seed: int = 42\n",
    "max_steps: int = 1000\n",
    "batch_size: BatchSizeArg = (10,\"words\")\n",
    "embedding_lr: float = 5e-5\n",
    "task_lr: float = 3e-4\n",
    "validation_interval: int = 10\n",
    "grad_max_norm: float = 5.0\n",
    "grad_accumulation_max_tokens: int = 96 * 128\n",
    "scorer: PseudoScorer\n",
    "output_dir: Optional[Path] = None\n",
    "cpu: bool = True\n",
    "\n",
    "#chargement de la config, soit avce resolve soit sans\n",
    "cfg = Config.from_disk(\"src/eds_pseudo/configs/config.cfg\")\n",
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49f96bd1-80a7-42b8-a6f4-5b4137f75253",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CamembertModel were not initialized from the model checkpoint at /export/home/amessager/projet/pseudo_champs_courts/data/eds-camembert-pseudo and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "\u001b[32m2025-09-26 14:59:41.991\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36medsnlp.data.json\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m65\u001b[0m - \u001b[1mFound 1 file under /export/home/amessager/projet/pseudo_champs_courts/data/training/private_small_train_data.jsonl\u001b[0m\n",
      "\u001b[32m2025-09-26 14:59:41.996\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36medsnlp.data.json\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m65\u001b[0m - \u001b[1mFound 1 file under /export/home/amessager/projet/pseudo_champs_courts/data/training/private_small_dev_data.jsonl\u001b[0m\n",
      "\u001b[32m2025-09-26 14:59:41.997\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36medsnlp.data.json\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m65\u001b[0m - \u001b[1mFound 1 file under /export/home/amessager/projet/pseudo_champs_courts/data/training/private_small_test_data.jsonl\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'vars': {'ml_spans': 'pseudo-ml',\n",
       "  'rb_spans': 'pseudo-rb',\n",
       "  'hybrid_spans': 'ents',\n",
       "  'limit': -1,\n",
       "  'labels': ['ADRESSE',\n",
       "   'DATE',\n",
       "   'DATE_NAISSANCE',\n",
       "   'IPP',\n",
       "   'MAIL',\n",
       "   'NDA',\n",
       "   'NOM',\n",
       "   'PRENOM',\n",
       "   'SECU',\n",
       "   'TEL',\n",
       "   'VILLE',\n",
       "   'ZIP']},\n",
       " 'nlp': Pipeline(lang=eds, pipes={\n",
       "   \"normalizer\": eds.normalizer,\n",
       "   \"simple-rules\": eds_pseudo.simple_rules,\n",
       "   \"addresses\": eds_pseudo.addresses,\n",
       "   \"context\": eds_pseudo.context,\n",
       "   \"ner\": eds.ner_crf,\n",
       "   \"clean\": eds_pseudo.clean,\n",
       "   \"merge\": eds_pseudo.merge,\n",
       "   \"dates-normalizer\": eds_pseudo.dates_normalizer\n",
       " }),\n",
       " 'components': {'normalizer': Draft[create_component],\n",
       "  'sentencizer': Draft[SentenceSegmenter],\n",
       "  'remove-lowercase': Draft[create_component],\n",
       "  'dates': Draft[PseudonymisationDates],\n",
       "  'simple-rules': Draft[Pseudonymisation],\n",
       "  'addresses': Draft[PseudonymisationAddresses],\n",
       "  'context': Draft[ContextMatcher],\n",
       "  'embedding': TextCnnEncoder(\n",
       "    (embedding): Transformer(\n",
       "      (transformer): CamembertModel(\n",
       "        (embeddings): CamembertEmbeddings(\n",
       "          (word_embeddings): Embedding(32006, 768, padding_idx=1)\n",
       "          (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "          (token_type_embeddings): Embedding(1, 768)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder): CamembertEncoder(\n",
       "          (layer): ModuleList(\n",
       "            (0-11): 12 x CamembertLayer(\n",
       "              (attention): CamembertAttention(\n",
       "                (self): CamembertSdpaSelfAttention(\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): CamembertSelfOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): CamembertIntermediate(\n",
       "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): CamembertOutput(\n",
       "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (pooler): CamembertPooler(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (activation): Tanh()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (module): TextCnn(\n",
       "      (convolutions): ModuleList(\n",
       "        (0): Conv1d(768, 768, kernel_size=(3,), stride=(1,))\n",
       "      )\n",
       "      (linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (residual): Residual()\n",
       "    )\n",
       "  ),\n",
       "  'ner': TrainableNerCrf(\n",
       "    (embedding): TextCnnEncoder(\n",
       "      (embedding): Transformer(\n",
       "        (transformer): CamembertModel(\n",
       "          (embeddings): CamembertEmbeddings(\n",
       "            (word_embeddings): Embedding(32006, 768, padding_idx=1)\n",
       "            (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "            (token_type_embeddings): Embedding(1, 768)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (encoder): CamembertEncoder(\n",
       "            (layer): ModuleList(\n",
       "              (0-11): 12 x CamembertLayer(\n",
       "                (attention): CamembertAttention(\n",
       "                  (self): CamembertSdpaSelfAttention(\n",
       "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (output): CamembertSelfOutput(\n",
       "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (intermediate): CamembertIntermediate(\n",
       "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                  (intermediate_act_fn): GELUActivation()\n",
       "                )\n",
       "                (output): CamembertOutput(\n",
       "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (pooler): CamembertPooler(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (activation): Tanh()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (module): TextCnn(\n",
       "        (convolutions): ModuleList(\n",
       "          (0): Conv1d(768, 768, kernel_size=(3,), stride=(1,))\n",
       "        )\n",
       "        (linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (residual): Residual()\n",
       "      )\n",
       "    )\n",
       "    (linear): Linear(in_features=768, out_features=0, bias=True)\n",
       "    (crf): MultiLabelBIOULDecoder()\n",
       "  ),\n",
       "  'clean': Draft[CleanEntities],\n",
       "  'merge': Draft[MergeEntities],\n",
       "  'dates-normalizer': Draft[DatesNormalizer]},\n",
       " 'scorers': {'rb_spans': 'pseudo-rb',\n",
       "  'ml_spans': 'pseudo-ml',\n",
       "  'hybrid_spans': 'ents',\n",
       "  'labels': ['ADRESSE',\n",
       "   'DATE',\n",
       "   'DATE_NAISSANCE',\n",
       "   'IPP',\n",
       "   'MAIL',\n",
       "   'NDA',\n",
       "   'NOM',\n",
       "   'PRENOM',\n",
       "   'SECU',\n",
       "   'TEL',\n",
       "   'VILLE',\n",
       "   'ZIP']},\n",
       " 'training_docs': {'randomize': True,\n",
       "  'max_length': 256,\n",
       "  'multi_sentence': True,\n",
       "  'limit': -1,\n",
       "  'source': Stream(\n",
       "    reader=JsonReader(path='/export/home/amessager/projet/pseudo_champs_courts/data/training/private_small_train_data.jsonl', shuffle=False, loop=False),\n",
       "    ops=[\n",
       "      map(<eds_pseudo.adapter.PseudoDict2DocConverter object at 0x7f4e37c2de20>)\n",
       "    ],\n",
       "    writer=None)},\n",
       " 'val_docs': {'source': Stream(\n",
       "    reader=JsonReader(path='/export/home/amessager/projet/pseudo_champs_courts/data/training/private_small_dev_data.jsonl', shuffle=False, loop=False),\n",
       "    ops=[\n",
       "      map(<eds_pseudo.adapter.PseudoDict2DocConverter object at 0x7f4e3c7b46e0>)\n",
       "    ],\n",
       "    writer=None)},\n",
       " 'test_docs': {'source': Stream(\n",
       "    reader=JsonReader(path='/export/home/amessager/projet/pseudo_champs_courts/data/training/private_small_test_data.jsonl', shuffle=False, loop=False),\n",
       "    ops=[\n",
       "      map(<eds_pseudo.adapter.PseudoDict2DocConverter object at 0x7f4e3c7b7200>)\n",
       "    ],\n",
       "    writer=None)},\n",
       " 'train': {'nlp': Pipeline(lang=eds, pipes={\n",
       "    \"normalizer\": eds.normalizer,\n",
       "    \"simple-rules\": eds_pseudo.simple_rules,\n",
       "    \"addresses\": eds_pseudo.addresses,\n",
       "    \"context\": eds_pseudo.context,\n",
       "    \"ner\": eds.ner_crf,\n",
       "    \"clean\": eds_pseudo.clean,\n",
       "    \"merge\": eds_pseudo.merge,\n",
       "    \"dates-normalizer\": eds_pseudo.dates_normalizer\n",
       "  }),\n",
       "  'max_steps': 10,\n",
       "  'validation_interval': 1,\n",
       "  'batch_size': '2000 words',\n",
       "  'embedding_lr': 5e-05,\n",
       "  'task_lr': 5e-05,\n",
       "  'seed': 43,\n",
       "  'scorer': {'rb_spans': 'pseudo-rb',\n",
       "   'ml_spans': 'pseudo-ml',\n",
       "   'hybrid_spans': 'ents',\n",
       "   'labels': ['ADRESSE',\n",
       "    'DATE',\n",
       "    'DATE_NAISSANCE',\n",
       "    'IPP',\n",
       "    'MAIL',\n",
       "    'NDA',\n",
       "    'NOM',\n",
       "    'PRENOM',\n",
       "    'SECU',\n",
       "    'TEL',\n",
       "    'VILLE',\n",
       "    'ZIP']},\n",
       "  'train_data': [{'randomize': True,\n",
       "    'max_length': 256,\n",
       "    'multi_sentence': True,\n",
       "    'limit': -1,\n",
       "    'source': Stream(\n",
       "      reader=JsonReader(path='/export/home/amessager/projet/pseudo_champs_courts/data/training/private_small_train_data.jsonl', shuffle=False, loop=False),\n",
       "      ops=[\n",
       "        map(<eds_pseudo.adapter.PseudoDict2DocConverter object at 0x7f4e37c2de20>)\n",
       "      ],\n",
       "      writer=None)}],\n",
       "  'val_data': {'source': Stream(\n",
       "     reader=JsonReader(path='/export/home/amessager/projet/pseudo_champs_courts/data/training/private_small_dev_data.jsonl', shuffle=False, loop=False),\n",
       "     ops=[\n",
       "       map(<eds_pseudo.adapter.PseudoDict2DocConverter object at 0x7f4e3c7b46e0>)\n",
       "     ],\n",
       "     writer=None)},\n",
       "  'grad_accumulation_max_tokens': 32000,\n",
       "  'output_dir': '/export/home/amessager/projet/pseudo_champs_courts/data/training/artifacts/'},\n",
       " 'evaluate': {'scorer': {'rb_spans': 'pseudo-rb',\n",
       "   'ml_spans': 'pseudo-ml',\n",
       "   'hybrid_spans': 'ents',\n",
       "   'labels': ['ADRESSE',\n",
       "    'DATE',\n",
       "    'DATE_NAISSANCE',\n",
       "    'IPP',\n",
       "    'MAIL',\n",
       "    'NDA',\n",
       "    'NOM',\n",
       "    'PRENOM',\n",
       "    'SECU',\n",
       "    'TEL',\n",
       "    'VILLE',\n",
       "    'ZIP']},\n",
       "  'dataset_name': 'AP-HP Pseudo Test',\n",
       "  'data': {'source': Stream(\n",
       "     reader=JsonReader(path='/export/home/amessager/projet/pseudo_champs_courts/data/training/private_small_test_data.jsonl', shuffle=False, loop=False),\n",
       "     ops=[\n",
       "       map(<eds_pseudo.adapter.PseudoDict2DocConverter object at 0x7f4e3c7b7200>)\n",
       "     ],\n",
       "     writer=None)}},\n",
       " 'package': {'name': 'eds-pseudo-public',\n",
       "  'hf_name': 'AP-HP/eds-pseudo-public'}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dans ce cas, la configuration est chargée grâce au module config et à tous les décorateurs qui permettent d'enregistrer les fonction dans le registre\n",
    "#par exemple, grâce à ces indications dans la config\n",
    "#[training_docs.source]\n",
    "#@readers = \"json\"  # or parquet or standoff\n",
    "#converter = \"pseudo\"  # defined in eds_pseudo/adapter.py\n",
    "#\n",
    "#confit a trouvé la classe qui permettait de lire la donnée, elel se trove dans le fichier adapater.py\n",
    "#et s'identifie grace à:\n",
    "#@registry.factory.register(\"eds.pseudo_dict2doc\", spacy_compatible=False)\n",
    "#class PseudoDict2DocConverter:\n",
    "#\n",
    "#le résultat se voit dans la cfg_resolved\n",
    "#'training_docs': {...\n",
    "#  'source': Stream(\n",
    "#    reader=JsonReader(path='/export/home/amessager/projet/pseudo_champs_courts/data/training/private_small_train_data.jsonl', shuffle=False, loop=False),\n",
    "#    ops=[\n",
    "#      map(<eds_pseudo.adapter.PseudoDict2DocConverter object at 0x7f4024eeafc0>)\n",
    "#    ],\n",
    "cfg_resolved = Config.from_disk(\"src/eds_pseudo/configs/config.cfg\").resolve(registry=edsnlp.registry)\n",
    "cfg_resolved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdfe80ac-6660-49b5-8a86-62c42cacecbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nlp=edsnlp.load(cfg_resolved)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f9b993-bf1a-43c6-bde1-9c95ffc0e3af",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2 - Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "938914cb-2169-49f2-a0f1-9dd2f8ae2530",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-26 15:59:03.777\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36medsnlp.data.json\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m65\u001b[0m - \u001b[1mFound 1 file under /export/home/amessager/projet/pseudo_champs_courts/data/training/private_small_dev_data.jsonl\u001b[0m\n",
      "\u001b[32m2025-09-26 15:59:03.781\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36medsnlp.data.json\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m65\u001b[0m - \u001b[1mFound 1 file under /export/home/amessager/projet/pseudo_champs_courts/data/training/private_small_train_data.jsonl\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#exemple de fichier:\n",
    "#{\"note_id\":\"synthetic_11831851499\",\"note_text\":\"krier franck rivaill59be@clinique-lyon.fr\",\"entities\":[{\"start\":0,\"end\":5,\"label\":\"NOM\"},{\"start\":6,\"end\":12,\"label\":\"PRENOM\"},{\"start\":13,\"end\":41,\"label\":\"MAIL\"}]}\n",
    "raw_val_docs = edsnlp.data.read_json(\n",
    "    cfg['val_docs']['source']['path'],#le chemin vers le fichier source, un jsonl, \n",
    "    converter=\"eds.pseudo_dict2doc\",#dans le fichier adapater.py, convertit en format doc le json, c.f. commentaire config pour plus d'infos\n",
    "    span_setter=\"pseudo-ml\",#très important, dans le fichier config, les entités à identifier doivent appartenir au span \"pseudo-ml\", sinon le preprocessing ne produira pas de targets\n",
    ")\n",
    "augmented_val_docs = PseudoReader(raw_val_docs)(nlp)\n",
    "\n",
    "raw_train_docs = edsnlp.data.read_json(\n",
    "    cfg['training_docs']['source']['path'],\n",
    "    converter=\"eds.pseudo_dict2doc\",\n",
    "    span_setter=\"pseudo-ml\",\n",
    ")\n",
    "augmented_train_docs = PseudoReader(raw_train_docs,max_length=15)(nlp)#la max length dans la config est a 256, mais mis a 15 pour faciliter la compréhension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "03aa4df6-297b-4d6f-8994-f8ccda3ecea8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-26 15:59:04.183\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36medsnlp.data.json\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m65\u001b[0m - \u001b[1mFound 1 file under /export/home/amessager/projet/pseudo_champs_courts/data/training/private_small_train_data.jsonl\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "krier franck rivaill59be@clinique-lyon.fr\n",
      "note_id: synthetic_11831851499\n",
      "entities ()\n",
      "span: {'pseudo-ml': [krier, franck, rivaill59be@clinique-lyon.fr]}\n",
      "comment les entitiés sont stockées dans le span: krier -> NOM\n"
     ]
    }
   ],
   "source": [
    "#illustration du span setter:\n",
    "raw_train_docs = edsnlp.data.read_json(\n",
    "    cfg['training_docs']['source']['path'],#le chemin vers le fichier source, un jsonl, \n",
    "    converter=\"eds.pseudo_dict2doc\",#dans le fichier adapater.py, convertit en format doc le json, c.f. commentaire config pour plus d'infos\n",
    "    span_setter=\"pseudo-ml\",#très important, dans le fichier config, les entités à identifier doivent appartenir au span \"pseudo-ml\", sinon le preprocessing ne produira pas de targets\n",
    ")\n",
    "l=list(raw_train_docs)#stream monté en mémoire\n",
    "print(l[0])\n",
    "print(\"note_id:\",l[0]._.note_id)\n",
    "print(\"entities\",l[0].ents)\n",
    "print(\"span:\",l[0].spans)\n",
    "print(\"comment les entitiés sont stockées dans le span:\",l[0].spans['pseudo-ml'][0],\"->\",l[0].spans['pseudo-ml'][0].label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "3b3c5d39-dd8b-4318-999d-78efef068083",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-26 15:59:07.625\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36medsnlp.data.json\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m65\u001b[0m - \u001b[1mFound 1 file under /export/home/amessager/projet/pseudo_champs_courts/data/training/private_small_train_data.jsonl\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "krier franck rivaill59be@clinique-lyon.fr\n",
      "note_id: synthetic_11831851499\n",
      "span: {'NOM': [krier], 'PRENOM': [franck], 'MAIL': [rivaill59be@clinique-lyon.fr]}\n",
      "entities (krier, franck, rivaill59be@clinique-lyon.fr)\n"
     ]
    }
   ],
   "source": [
    "#illustration du span setter:\n",
    "raw_train_docs = edsnlp.data.read_json(\n",
    "    cfg['training_docs']['source']['path'],#le chemin vers le fichier source, un jsonl, \n",
    "    converter=\"eds.pseudo_dict2doc\",#dans le fichier adapater.py, convertit en format doc le json, c.f. commentaire config pour plus d'infos\n",
    "    #span_setter=\"pseudo-ml\",#très important, dans le fichier config, les entités à identifier doivent appartenir au span \"pseudo-ml\", sinon le preprocessing ne produira pas de targets\n",
    ")\n",
    "#dans ces conditions, il n'y aura pas d'apprentissage parce que il n'y a pas de span pseudo-ml\n",
    "l=list(raw_train_docs)\n",
    "print(l[0])\n",
    "print(\"note_id:\",l[0]._.note_id)\n",
    "print(\"span:\",l[0].spans)\n",
    "print(\"entities\",l[0].ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "5ae0dbde-c56a-430e-8739-6c637f67053b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner\n",
      "{'pseudo-ml': True}\n"
     ]
    }
   ],
   "source": [
    "ner=nlp.pipeline[4]#la phase de NER (CNN(Transformer))\n",
    "print(ner[0])\n",
    "print(ner[1].target_span_getter)#c'est ici que l'on voit ou sont définis les spans rercherchés pour les targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "69049be7-82c6-4a7c-9899-adf2d14ed429",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "edsnlp.pipes.trainable.ner_crf.ner_crf.TrainableNerCrf"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(ner[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "7cd11ff1-7a27-45a1-96ef-389f5073ba3e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainableNerCrf(\n",
       "  (embedding): TextCnnEncoder(\n",
       "    (embedding): Transformer(\n",
       "      (transformer): CamembertModel(\n",
       "        (embeddings): CamembertEmbeddings(\n",
       "          (word_embeddings): Embedding(32006, 768, padding_idx=1)\n",
       "          (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "          (token_type_embeddings): Embedding(1, 768)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder): CamembertEncoder(\n",
       "          (layer): ModuleList(\n",
       "            (0-11): 12 x CamembertLayer(\n",
       "              (attention): CamembertAttention(\n",
       "                (self): CamembertSdpaSelfAttention(\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): CamembertSelfOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): CamembertIntermediate(\n",
       "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): CamembertOutput(\n",
       "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (pooler): CamembertPooler(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (activation): Tanh()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (module): TextCnn(\n",
       "      (convolutions): ModuleList(\n",
       "        (0): Conv1d(768, 768, kernel_size=(3,), stride=(1,))\n",
       "      )\n",
       "      (linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (residual): Residual()\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=768, out_features=60, bias=True)\n",
       "  (crf): MultiLabelBIOULDecoder()\n",
       ")"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner[1]#c'est un nn.module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "e6079ea2-97f3-435f-9e79-22224fea9332",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-26 15:59:11.642\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36medsnlp.data.json\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m65\u001b[0m - \u001b[1mFound 1 file under /export/home/amessager/projet/pseudo_champs_courts/data/training/private_small_train_data.jsonl\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Remarque importante: \n",
    "#lorsque le raw_train_docs est défini, c'est un stream, cad qu'il n'est pas chargé avant qu'il soit appelé, un peu comme un iterator\n",
    "#pour que edsnlp puisse tokenizer les mots, la librairie edsnlp a besoin de définir un vocabulaire, l'ensmeble du vocabualire n'est pas \n",
    "#prérempli (il y aurait trop de mo) mais construit au fur et à mesure que les mots sont découverts\n",
    "#donc si raw_train_docs est chargé en mémoire, avec une list() par exemple, le vocabulaire est construit a partir du corpus, \n",
    "#et ne sera pas nécessairement le meme que celui du pipeline nlp\n",
    "#c'est pratiquement la seule raison pour laquelle le pipeline nlp est appelé lors de construction du texte d'entrainement\n",
    "raw_train_docs = edsnlp.data.read_json(\n",
    "    cfg['training_docs']['source']['path'],\n",
    "    converter=\"eds.pseudo_dict2doc\",\n",
    "    span_setter=\"pseudo-ml\",\n",
    ")\n",
    "augmented_train_docs = PseudoReader(list(raw_train_docs),max_length=15)(nlp)\n",
    "l=list(augmented_train_docs)#stream chargé en mémoire\n",
    "l[0].vocab == nlp.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "371213e7-f652-4951-94db-c70d13b7aed0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-26 15:59:12.161\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36medsnlp.data.json\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m65\u001b[0m - \u001b[1mFound 1 file under /export/home/amessager/projet/pseudo_champs_courts/data/training/private_small_train_data.jsonl\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train_docs = edsnlp.data.read_json(\n",
    "    cfg['training_docs']['source']['path'],\n",
    "    converter=\"eds.pseudo_dict2doc\",\n",
    "    span_setter=\"pseudo-ml\",\n",
    ")\n",
    "augmented_train_docs = PseudoReader(raw_train_docs,max_length=15)(nlp)\n",
    "l=list(augmented_train_docs)#stream chargé en mémoire\n",
    "l[0].vocab == nlp.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "18fdb668-31de-4982-8c52-12ab10befa34",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mavoungou bouanga cecile Admission en service de gériatrie Besoin d'aide matérielle urgent\n"
     ]
    }
   ],
   "source": [
    "#Note: on peut remarquer que le texte produit par le PseudoReader est maintenant composé de \"phrase\" de taille max 15\n",
    "#le deuxieme exemple dans le raw_train_doc est beaucoup plus long\n",
    "print(l[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3cb394-715e-4876-baad-6459bac5fc62",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 3 - Extraction des targets et créations des tenseurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "4316c84c-8deb-441c-a5ed-0819b11d849d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-26 15:59:13.296\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36medsnlp.data.json\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m65\u001b[0m - \u001b[1mFound 1 file under /export/home/amessager/projet/pseudo_champs_courts/data/training/private_small_train_data.jsonl\u001b[0m\n",
      "133it [00:00, 1371.09it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ner/embedding/embedding/input_ids': [[1482,\n",
       "   4420,\n",
       "   6125,\n",
       "   496,\n",
       "   3366,\n",
       "   1872,\n",
       "   4303,\n",
       "   3853,\n",
       "   1138,\n",
       "   2266,\n",
       "   216,\n",
       "   1660,\n",
       "   1120,\n",
       "   26,\n",
       "   1107,\n",
       "   88,\n",
       "   9,\n",
       "   427]],\n",
       " 'ner/embedding/embedding/word_tokens': [[0,\n",
       "   1,\n",
       "   2,\n",
       "   3,\n",
       "   4,\n",
       "   5,\n",
       "   6,\n",
       "   7,\n",
       "   8,\n",
       "   9,\n",
       "   10,\n",
       "   11,\n",
       "   12,\n",
       "   13,\n",
       "   14,\n",
       "   15,\n",
       "   16,\n",
       "   17]],\n",
       " 'ner/embedding/embedding/word_lengths': [[2, 2, 3, 1, 1, 1, 3, 1, 2, 1, 1]],\n",
       " 'ner/embedding/embedding/prompts': [[]],\n",
       " 'ner/embedding/embedding/stats/tokens': 18,\n",
       " 'ner/embedding/embedding/stats/words|ner/stats/ner_words': 11,\n",
       " 'ner/embedding/embedding/stats/contexts': 1,\n",
       " 'ner/lengths': [11],\n",
       " 'ner/targets': [[[0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0]]]}"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train_docs = edsnlp.data.read_json(\n",
    "    cfg['training_docs']['source']['path'],\n",
    "    converter=\"eds.pseudo_dict2doc\",\n",
    "    span_setter=\"pseudo-ml\",\n",
    ")\n",
    "augmented_train_docs = PseudoReader(raw_train_docs,max_length=15)(nlp)\n",
    "\n",
    "#nlp.post_init sert à finaliser l'initialisation du pipeline NLP en lui montrant un échantillon des données d'entraînement.\n",
    "#Cette étape permet aux différents composants du pipeline, comme le vocabulaire et le tokenizer,\n",
    "#de s'adapter aux données spécifiques qu'ils vont traiter avant que l'entraînement ne commence réellement.\n",
    "#cela permet nottamment de regarder le nombre de classe à traiter et de définir en conséquence le nombre de têtes classificatrices\n",
    "#donc la taille du modèle de sortie\n",
    "nlp.post_init(augmented_train_docs)\n",
    "\n",
    "preprocessed = list(\n",
    "    nlp.preprocess_many(augmented_train_docs, supervision=True).set_processing(\n",
    "        show_progress=True\n",
    "    )\n",
    ")\n",
    "\n",
    "preprocessed[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "eb783dbd-80b5-427b-932f-1dde7aa0bf4d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pseudo-ml': [krier, franck, rivaill59be@clinique-lyon.fr], 'pseudo-rb': [krier, franck, rivaill59be@clinique-lyon.fr], 'pollutions': [rivaill59be@clinique-lyon.fr]}"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "98b18027-980a-4414-9818-f96e942a9171",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NOM-4',\n",
       " 'PRENOM-4',\n",
       " 'MAIL-2',\n",
       " 'MAIL-1',\n",
       " 'MAIL-1',\n",
       " 'MAIL-1',\n",
       " 'MAIL-1',\n",
       " 'MAIL-1',\n",
       " 'MAIL-1',\n",
       " 'MAIL-1',\n",
       " 'MAIL-3']"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Il faut remarquer que le preprocessing a créer les targets selon la méthode BIOUL:\n",
    "#1 signifie inside\n",
    "#2 signifie beginning\n",
    "#3 signifie end\n",
    "#4 signifie entité composée d\"un seul token\n",
    "#la pharse est toujours la meme: krier franck rivaill59be@clinique-lyon.fr\n",
    "#NOM (token unique=4) PRENOM (token unique=4) MAIL_début (token=2) MAIL_inside (token=1) MAIL_inside (token=1) ... MAIL_inside (token=1) MAIL_fin (token=3)\n",
    "labels=nlp.pipes.ner.labels\n",
    "target=preprocessed[0]['ner/targets'][0]\n",
    "[labels[j]+f'-{target[i][j]}' for i in range(len(target)) for j in range(len(target[0])) if target[i][j]>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "1bcad290-bfc5-4a9e-9dd3-06d1ea4b0261",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ner': {'embedding': {'embedding': {'input_ids': [[1482,\n",
       "      4420,\n",
       "      6125,\n",
       "      496,\n",
       "      3366,\n",
       "      1872,\n",
       "      4303,\n",
       "      3853,\n",
       "      1138,\n",
       "      2266,\n",
       "      216,\n",
       "      1660,\n",
       "      1120,\n",
       "      26,\n",
       "      1107,\n",
       "      88,\n",
       "      9,\n",
       "      427]],\n",
       "    'word_tokens': [[0,\n",
       "      1,\n",
       "      2,\n",
       "      3,\n",
       "      4,\n",
       "      5,\n",
       "      6,\n",
       "      7,\n",
       "      8,\n",
       "      9,\n",
       "      10,\n",
       "      11,\n",
       "      12,\n",
       "      13,\n",
       "      14,\n",
       "      15,\n",
       "      16,\n",
       "      17]],\n",
       "    'word_lengths': [[2, 2, 3, 1, 1, 1, 3, 1, 2, 1, 1]],\n",
       "    'prompts': [[]],\n",
       "    'stats': {'tokens': 18, 'words': 11, 'contexts': 1}}},\n",
       "  'lengths': [11],\n",
       "  '$contexts': [krier franck rivaill59be@clinique-lyon.fr],\n",
       "  'stats': {'ner_words': 11},\n",
       "  'targets': [[[0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0],\n",
       "    [0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0],\n",
       "    [0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0],\n",
       "    [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "    [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "    [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "    [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "    [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "    [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "    [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "    [0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0]]]}}"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dans le code c'est fait de la facon suivante\n",
    "#https://github.com/aphp/edsnlp/blob/c7ae3441d39450749d6225c25596da666a374d6e/edsnlp/pipes/trainable/ner_crf/ner_crf.py#L402\n",
    "augmented_train_docs = PseudoReader(raw_train_docs,max_length=15)(nlp)\n",
    "prep={}\n",
    "supervision=True\n",
    "for name, component in nlp.pipeline:\n",
    "    if name not in \"disabled\":\n",
    "        prep_comp = (\n",
    "            component.preprocess_supervised(list(augmented_train_docs)[0])\n",
    "            if supervision and hasattr(component, \"preprocess_supervised\")\n",
    "            else component.preprocess(list(augmented_train_docs)[0])\n",
    "            if hasattr(component, \"preprocess\")\n",
    "            else None\n",
    "        )\n",
    "        if prep_comp is not None:\n",
    "            prep[name] = prep_comp\n",
    "prep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3a47c5-c8a9-4e27-b852-8c58ea021d42",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 4 creation des batchs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "b97a4349-cacb-4c49-9fb5-c01231c0044f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (transformer): CamembertModel(\n",
       "    (embeddings): CamembertEmbeddings(\n",
       "      (word_embeddings): Embedding(32006, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): CamembertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x CamembertLayer(\n",
       "          (attention): CamembertAttention(\n",
       "            (self): CamembertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): CamembertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): CamembertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): CamembertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): CamembertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#le transformer a proprement parlé\n",
    "trf_pipe = next(\n",
    "        module\n",
    "        for name, pipe in nlp.torch_components()\n",
    "        for module_name, module in pipe.named_component_modules()\n",
    "        if isinstance(module, Transformer)\n",
    "    )\n",
    "trf_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "055e1358-67d9-425a-bf6b-f93755f2e1a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 'words')\n"
     ]
    }
   ],
   "source": [
    "print(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "88f7d037-7c81-46af-8a3e-0fb76a346a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LengthSortedBatchSampler\n",
    "#Son but est d'accélérer l'entraînement en minimisant le \"padding\" (bourrage).\n",
    "#Pour cela, il trie tous les documents par longueur avant de les grouper.\n",
    "#Il crée ainsi des lots (batchs) contenant des textes de tailles similaires.\n",
    "#Cela rend le traitement par les modèles de type transformers plus efficace.\n",
    "#Enfin, il mélange légèrement les lots pour conserver de l'aléa dans l'entraînement.\n",
    "#\n",
    "#SubBatchCollater\n",
    "#Son rôle est d'éviter les erreurs de mémoire sur le GPU (\"out of memory\").\n",
    "#Il prend un lot (batch) et le divise en \"mini-lots\" si nécessaire.\n",
    "#La division se produit si le nombre total de tokens du lot dépasse un seuil.\n",
    "#Chaque mini-lot est ainsi garanti de ne pas surcharger la mémoire.\n",
    "#Il formate ensuite chaque mini-lot pour qu'il soit prêt pour le modèle.\n",
    "\n",
    "#les deux classes sont dans le fichier train.py pour plus d'infos\n",
    "dataloader = DataLoader(\n",
    "        preprocessed,\n",
    "        batch_sampler=LengthSortedBatchSampler(\n",
    "            preprocessed,\n",
    "            batch_size=batch_size[0],\n",
    "            batch_unit=batch_size[1],\n",
    "        ),\n",
    "        collate_fn=SubBatchCollater(\n",
    "            nlp,\n",
    "            trf_pipe,\n",
    "            grad_accumulation_max_tokens=grad_accumulation_max_tokens,\n",
    "        ),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "24bf04b7-1b9d-44da-a314-d6ad79581eb8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'ner': {'embedding': {'embedding': {'input_ids': FoldedTensor([[    5,    30, 10674,  5455, 12811,    15, 11498, 22748,   275,\n",
       "                     7392,    15,  4415,     6]]),\n",
       "     'word_offsets': FoldedTensor([ 0,  1,  4,  5,  6,  7,  9, 10]),\n",
       "     'word_indices': tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]),\n",
       "     'empty_word_indices': tensor([], dtype=torch.int64),\n",
       "     'stats': {'tokens': 11, 'words': 8, 'contexts': 1},\n",
       "     '__cache_key__': ('collate',\n",
       "      'Transformer<139974535377888>',\n",
       "      -3107237143443861468)},\n",
       "    '__cache_key__': ('collate',\n",
       "     'TextCnnEncoder<139974535296208>',\n",
       "     490225863448190307)},\n",
       "   'stats': {'ner_words': 8},\n",
       "   'targets': tensor([[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0],\n",
       "            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]]),\n",
       "   '__cache_key__': ('collate',\n",
       "    'TrainableNerCrf<139974533303136>',\n",
       "    -435262411305880681)}}]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch=next(iter(dataloader))\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e42d04d-d9b8-42f9-be10-951f086bad66",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 5 - optimiseur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "cd753b94-2d16-4c8a-98cf-f7c942412701",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ner\n",
      "Optimizing:\n",
      " - 6 params (2406972 total)\n",
      " - 200 params (110623488 total)\n",
      "Not optimizing 0 params\n"
     ]
    }
   ],
   "source": [
    "trained_pipes = nlp.torch_components()\n",
    "print(\"Training\", \", \".join([name for name, c in trained_pipes]))\n",
    "\n",
    "trf_params = set(trf_pipe.parameters())\n",
    "params = set(nlp.parameters())\n",
    "optimizer = ScheduledOptimizer(\n",
    "    torch.optim.AdamW(\n",
    "        [\n",
    "            {\n",
    "                \"params\": list(params - trf_params),\n",
    "                \"lr\": task_lr,\n",
    "                \"schedules\": [\n",
    "                    LinearSchedule(\n",
    "                        total_steps=max_steps,\n",
    "                        warmup_rate=0.1,\n",
    "                        start_value=task_lr,\n",
    "                        path=\"lr\",\n",
    "                    )\n",
    "                ],\n",
    "            },\n",
    "            {\n",
    "                \"params\": list(trf_params),\n",
    "                \"lr\": embedding_lr,\n",
    "                \"schedules\": [\n",
    "                    LinearSchedule(\n",
    "                        total_steps=max_steps,\n",
    "                        warmup_rate=0.1,\n",
    "                        start_value=0,\n",
    "                        path=\"lr\",\n",
    "                    )\n",
    "                ],\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "grad_params = {p for group in optimizer.param_groups for p in group[\"params\"]}\n",
    "print(\n",
    "    \"Optimizing:\"\n",
    "    + \"\".join(\n",
    "        f\"\\n - {len(group['params'])} params \"\n",
    "        f\"({sum(p.numel() for p in group['params'])} total)\"\n",
    "        for group in optimizer.param_groups\n",
    "    )\n",
    ")\n",
    "print(f\"Not optimizing {len(params - grad_params)} params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "78c603b8-bd60-4ea2-ae07-f0457562aa14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "accelerator = Accelerator(cpu=cpu)\n",
    "trained_pipes = dict(nlp.torch_components())\n",
    "print(\"Device:\", accelerator.device)\n",
    "[dataloader, optimizer, *accelerated_pipes] = accelerator.prepare(\n",
    "    dataloader,\n",
    "    optimizer,\n",
    "    *trained_pipes.values(),\n",
    ")\n",
    "trained_pipes = list(zip(trained_pipes.keys(), accelerated_pipes))\n",
    "del accelerated_pipes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d224137e-fdab-4871-89f5-8bf9b3ac0140",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 6 - calculs des metriques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "400739bf-f522-4c7a-ba36-4d84dbfb1d32",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<confit.utils.random.set_seed at 0x7f4fc3a30e60>"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from confit.utils.random import set_seed\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "dc784c3a-af5a-447f-8081-3c34c81daf91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scorer=PseudoScorer(**cfg_resolved['scorers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "73b9e083-3a83-43fa-9802-8e75a9dd72d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ents'"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#la méthode par défaut pour mesurer la précision et la sensitivé est la méthode hybride\n",
    "#il y a originellement deux méthodes: une basées uniquement sur le transformer et l'autre basée sur des règles\n",
    "#la méthode hybride est une hybridation de ces deux méthodes\n",
    "#le scorer utilise cette méthode et par défaut le span utilisé est ents\n",
    "#il faut donc recharger les données avec aussi le span ents\n",
    "scorer.hybrid_spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "cf899d68-e30a-48d1-8d87-f8c2309966d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-26 15:59:17.473\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36medsnlp.data.json\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m65\u001b[0m - \u001b[1mFound 1 file under /export/home/amessager/projet/pseudo_champs_courts/data/training/private_small_train_data.jsonl\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "krier franck rivaill59be@clinique-lyon.fr"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train_docs = edsnlp.data.read_json(\n",
    "    cfg['training_docs']['source']['path'],\n",
    "    converter=\"eds.pseudo_dict2doc\",\n",
    "    span_setter=[\"pseudo-ml\", \"pseudo-rb\", \"ents\"],\n",
    ")\n",
    "augmented_train_docs = PseudoReader(raw_train_docs,max_length=15)(nlp)\n",
    "docs=list(augmented_train_docs)\n",
    "doc=docs[0]\n",
    "docs=[doc]#simplification a 1 doc\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "6874a551-f2ed-4983-9ea4-2d8ad2aea5d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(krier, franck, rivaill59be@clinique-lyon.fr)\n",
      "{'pseudo-ml': [krier, franck, rivaill59be@clinique-lyon.fr], 'pseudo-rb': [krier, franck, rivaill59be@clinique-lyon.fr], 'pollutions': [rivaill59be@clinique-lyon.fr]}\n"
     ]
    }
   ],
   "source": [
    "#On voit bien que les entités sont dans les ents et dans les spans pseudo-ml et pseudo-rb\n",
    "print(doc.ents)\n",
    "print(doc.spans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "598acc1a-6fa4-4929-a694-4e06946f7a1c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  5.56it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'doc_annotation': {'cats': {}, 'entities': ['U-NOM', 'U-PRENOM', 'B-MAIL', 'I-MAIL', 'I-MAIL', 'I-MAIL', 'I-MAIL', 'I-MAIL', 'I-MAIL', 'I-MAIL', 'L-MAIL'], 'spans': {'pseudo-ml': [(0, 5, 'NOM', ''), (6, 12, 'PRENOM', ''), (13, 41, 'MAIL', '')], 'pseudo-rb': [(0, 5, 'NOM', ''), (6, 12, 'PRENOM', ''), (13, 41, 'MAIL', '')], 'pollutions': [(13, 41, 'web', '')]}, 'links': {}}, 'token_annotation': {'ORTH': ['krier', 'franck', 'rivaill', '59', 'be', '@', 'clinique', '-', 'lyon', '.', 'fr'], 'SPACY': [True, True, False, False, False, False, False, False, False, False, False], 'TAG': ['', '', 'EXCLUDED', 'EXCLUDED', 'EXCLUDED', 'EXCLUDED', 'EXCLUDED', 'EXCLUDED', 'EXCLUDED', 'EXCLUDED', 'EXCLUDED'], 'LEMMA': ['', '', '', '', '', '', '', '', '', '', ''], 'POS': ['', '', '', '', '', '', '', '', '', '', ''], 'MORPH': ['', '', '', '', '', '', '', '', '', '', ''], 'HEAD': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 'DEP': ['', '', '', '', '', '', '', '', '', '', ''], 'SENT_START': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}}]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from edsnlp.scorers import make_examples\n",
    "#on nettoie pour faire la prédiction et ne pas tacher le processus\n",
    "clean_docs: List[spacy.tokens.Doc] = [d.copy() for d in [doc]]\n",
    "for d in clean_docs:\n",
    "    d.ents = []\n",
    "    d.spans.clear()\n",
    "preds=list(nlp.pipe(clean_docs).set_processing(show_progress=True))\n",
    "#Les objets examples sont la structure de données standard pour évaluer un modèle.\n",
    "#Chaque Example est une paire qui apparie la vérité terrain (le document original doc avec les bonnes annotations) avec la prédiction du modèle (preds).\n",
    "#Cette mise en correspondance est essentielle pour les fonctions d'évaluation (scorers).\n",
    "#Elles peuvent ainsi facilement confronter la \"bonne réponse\" et la \"copie de l'élève\".\n",
    "#Cela leur permet de calculer des métriques comme la précision, le rappel et le F1-score.\n",
    "examples = make_examples([doc], preds)\n",
    "examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "b13173b0-b135-4aaa-80d5-a125a57c8dbb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pollutions': [rivaill59be@clinique-lyon.fr], 'pseudo-rb': [rivaill59be@clinique-lyon.fr], 'pseudo-ml': [krier franck rivaill59be@clinique-lyon.fr], 'ents': [], '*': [], 'PRENOM': [krier franck rivaill59be@clinique-lyon.fr]}"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#le process prédit du prénom pour tous les tokens\n",
    "examples[0].predicted.spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "eb37d4ee-2560-4859-8612-e225433ed01c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pseudo-ml': [krier, franck, rivaill59be@clinique-lyon.fr], 'pseudo-rb': [krier, franck, rivaill59be@clinique-lyon.fr], 'pollutions': [rivaill59be@clinique-lyon.fr]}"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#alors qu'il y avait 3 entités à prédire\n",
    "examples[0].reference.spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "7e6829f8-5ced-4eb0-a0e8-7696f42c0309",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'micro': {'f': 0.09090909090909091,\n",
       "  'p': 0.09090909090909091,\n",
       "  'r': 0.09090909090909091,\n",
       "  'tp': 1,\n",
       "  'support': 11,\n",
       "  'positives': 11},\n",
       " 'PRENOM': {'f': 0.16666666666666666,\n",
       "  'p': 0.09090909090909091,\n",
       "  'r': 1,\n",
       "  'tp': 1,\n",
       "  'support': 1,\n",
       "  'positives': 11},\n",
       " 'NOM': {'f': 0.0, 'p': 1, 'r': 0.0, 'tp': 0, 'support': 1, 'positives': 0},\n",
       " 'MAIL': {'f': 0.0, 'p': 1, 'r': 0.0, 'tp': 0, 'support': 9, 'positives': 0}}"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#la précision et le reacll pour la classe PRENOM est logique (un TP et 10 FP)\n",
    "from edsnlp.scorers.ner import ner_exact_scorer, ner_token_scorer\n",
    "token_scores = ner_token_scorer(examples,span_getter={'pseudo-ml':True})\n",
    "token_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "1fafcdc9-e093-4172-9f82-ee88d80d17e2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'micro': {'r': 1, 'full': 1.0, 'tp': 11, 'support': 11},\n",
       " 'NOM': {'r': 1, 'full': 1.0, 'tp': 1, 'support': 1},\n",
       " 'PRENOM': {'r': 1, 'full': 1.0, 'tp': 1, 'support': 1},\n",
       " 'MAIL': {'r': 1, 'full': 1.0, 'tp': 9, 'support': 9}}"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.eds_pseudo.eds_pseudo.scorer import redact_scorer\n",
    "#Le redact_score est une métrique d'évaluation conçue spécifiquement pour les tâches de pseudonymisation ou d'anonymisation de texte.\n",
    "#Son objectif principal n'est pas de savoir si un modèle a correctement classifié une information (ex: un NOM en tant que NOM), \n",
    "#mais de mesurer son efficacité à détecter toutes les informations sensibles qui doivent être masquées, peu importe leur catégorie.\n",
    "#toutes les entités ont été détectées d'ou le bon score\n",
    "redact_scores = redact_scorer(examples,span_getter=ner[1].target_span_getter)\n",
    "redact_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "b390a39a-1153-4638-8846-5cdd53a92c29",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-26 15:59:19.178\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36medsnlp.data.json\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m65\u001b[0m - \u001b[1mFound 1 file under /export/home/amessager/projet/pseudo_champs_courts/data/training/private_small_train_data.jsonl\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "krier franck rivaill59be@clinique-lyon.fr"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train_docs = edsnlp.data.read_json(\n",
    "    cfg['training_docs']['source']['path'],\n",
    "    converter=\"eds.pseudo_dict2doc\",\n",
    "    span_setter=[\"pseudo-ml\", \"pseudo-rb\", \"ents\"],\n",
    ")\n",
    "augmented_train_docs = PseudoReader(raw_train_docs,max_length=15)(nlp)\n",
    "docs=list(augmented_train_docs)\n",
    "doc=docs[0]\n",
    "docs=[doc]\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "51508242-4bef-4b89-b564-aab8af58c133",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  6.84it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'type': 'precision',\n",
       "  'name': 'Token Scores / PRENOM / Precision',\n",
       "  'value': 0.09090909090909091},\n",
       " {'type': 'recall', 'name': 'Token Scores / PRENOM / Recall', 'value': 1},\n",
       " {'type': 'f1',\n",
       "  'name': 'Token Scores / PRENOM / F1',\n",
       "  'value': 0.16666666666666666},\n",
       " {'type': 'recall', 'name': 'Token Scores / PRENOM / Redact', 'value': 1},\n",
       " {'type': 'accuracy',\n",
       "  'name': 'Token Scores / PRENOM / Redact Full',\n",
       "  'value': 1.0}]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics=scorer(nlp,[doc])\n",
    "[m for m in metrics if  \"PRENOM\" in m['name']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0c4ff7-603a-4338-9cfd-b30fd7d42234",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 7 Entrainement - 1 seul étape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "8a4eebc6-1fd8-41b1-9818-717bd19aec6c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<edsnlp.core.pipeline.Pipeline.train.<locals>.context at 0x7f4e1fffaf90>"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cumulated_data = defaultdict(lambda: 0.0, count=0)\n",
    "\n",
    "iterator = itertools.chain.from_iterable(itertools.repeat(dataloader))\n",
    "all_metrics = []\n",
    "nlp.train(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "86e39d12-e38a-46d7-8dc5-6cc4a34e9af8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ner': {'embedding': {'embedding': {'input_ids': FoldedTensor([[    5,  8278, 14921,    54,   379,    18,    11,   761,    30,\n",
       "                    8399,  2997,  5916,    14,   586,  8688,     6]]),\n",
       "    'word_offsets': FoldedTensor([ 0,  1,  2,  3,  4,  6,  7,  8,  9, 11, 12, 13]),\n",
       "    'word_indices': tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14]),\n",
       "    'empty_word_indices': tensor([], dtype=torch.int64),\n",
       "    'stats': {'tokens': 14, 'words': 12, 'contexts': 1},\n",
       "    '__cache_key__': ('collate',\n",
       "     'Transformer<139974535377888>',\n",
       "     3668150743000692910)},\n",
       "   '__cache_key__': ('collate',\n",
       "    'TextCnnEncoder<139974535296208>',\n",
       "    -4300469708591893696)},\n",
       "  'stats': {'ner_words': 12},\n",
       "  'targets': tensor([[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "           [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "           [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "           [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "           [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "           [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "           [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "           [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "           [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "           [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "           [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "           [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]]),\n",
       "  '__cache_key__': ('collate',\n",
       "   'TrainableNerCrf<139974533303136>',\n",
       "   8239342313750516940)}}"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iterator)\n",
    "print(len(batch))#1 seul minibatch par batch, c'est parce que le grad_accumulation token = 32000 >> 11 token par batch\n",
    "mini_batch=batch[0]\n",
    "mini_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "8067227b-891f-47c1-a785-b5ef19334b51",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': tensor(13.3557, grad_fn=<DivBackward0>), 'tags': None, 'probs': None}"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_pipe=nlp.pipeline[4][1]#le 4ème element du pipeline\n",
    "ner_pipe.forward(mini_batch['ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "0186312d-cff6-490c-829d-a1aa3ed75353",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': tensor(13.6573, grad_fn=<DivBackward0>), 'tags': None, 'probs': None}"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_pipe.module_forward(mini_batch['ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "81739836-ea02-436e-8474-3b7d894580ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer.zero_grad()\n",
    "loss = torch.zeros((), device=accelerator.device)\n",
    "with nlp.cache():\n",
    "    for name, pipe in trained_pipes:\n",
    "        output = pipe.module_forward(mini_batch[name])\n",
    "        if \"loss\" in output:\n",
    "            loss += output[\"loss\"]\n",
    "        for key, value in output.items():\n",
    "            if key.endswith(\"loss\"):\n",
    "                cumulated_data[key] += float(value)\n",
    "accelerator.backward(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "22a46743-d744-4fbd-a53a-72db6d75fcdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>()>,\n",
       "            {'count': 0, 'loss': 13.80490493774414})"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cumulated_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "919fa922-c978-425a-b3ac-a376b27cc5df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'embedding': {'embedding': {'input_ids': FoldedTensor([[    5,  8278, 14921,    54,   379,    18,    11,   761,    30,\n",
       "                   8399,  2997,  5916,    14,   586,  8688,     6]]),\n",
       "   'word_offsets': FoldedTensor([ 0,  1,  2,  3,  4,  6,  7,  8,  9, 11, 12, 13]),\n",
       "   'word_indices': tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14]),\n",
       "   'empty_word_indices': tensor([], dtype=torch.int64),\n",
       "   'stats': {'tokens': 14,\n",
       "    'words': 12,\n",
       "    'contexts': 1,\n",
       "    '__batch_hash__': 203825776226659863},\n",
       "   '__cache_key__': ('collate',\n",
       "    'Transformer<139974535377888>',\n",
       "    3668150743000692910),\n",
       "   '__batch_hash__': -182304495090182323},\n",
       "  '__cache_key__': ('collate',\n",
       "   'TextCnnEncoder<139974535296208>',\n",
       "   -4300469708591893696),\n",
       "  '__batch_hash__': -1749017405201664493},\n",
       " 'stats': {'ner_words': 12, '__batch_hash__': -8674139348251538902},\n",
       " 'targets': tensor([[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]]),\n",
       " '__cache_key__': ('collate',\n",
       "  'TrainableNerCrf<139974533303136>',\n",
       "  8239342313750516940),\n",
       " '__batch_hash__': 6505878401210426879}"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mini_batch['ner']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "5197aa0f-f82d-4980-b720-09dd618275df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainableNerCrf(\n",
       "  (embedding): TextCnnEncoder(\n",
       "    (embedding): Transformer(\n",
       "      (transformer): CamembertModel(\n",
       "        (embeddings): CamembertEmbeddings(\n",
       "          (word_embeddings): Embedding(32006, 768, padding_idx=1)\n",
       "          (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "          (token_type_embeddings): Embedding(1, 768)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder): CamembertEncoder(\n",
       "          (layer): ModuleList(\n",
       "            (0-11): 12 x CamembertLayer(\n",
       "              (attention): CamembertAttention(\n",
       "                (self): CamembertSdpaSelfAttention(\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): CamembertSelfOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): CamembertIntermediate(\n",
       "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): CamembertOutput(\n",
       "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (pooler): CamembertPooler(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (activation): Tanh()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (module): TextCnn(\n",
       "      (convolutions): ModuleList(\n",
       "        (0): Conv1d(768, 768, kernel_size=(3,), stride=(1,))\n",
       "      )\n",
       "      (linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (residual): Residual()\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=768, out_features=60, bias=True)\n",
       "  (crf): MultiLabelBIOULDecoder()\n",
       ")"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_pipe = nlp.pipes.ner\n",
    "#pour voir les tenseurs qui se propagent dans le NER\n",
    "ner_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "e3aff50b-8f5a-4541-9ba2-f69052ea256c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'embeddings': FoldedTensor([[[-2.0744,  0.3438,  1.1768,  ..., -0.9984,  0.2379,  0.8769],\n",
       "                [-1.6560,  0.7949,  1.5319,  ..., -0.6945, -0.3689,  1.1196],\n",
       "                [-3.3010,  0.6003,  0.7830,  ..., -1.4142, -1.6868,  2.0221],\n",
       "                ...,\n",
       "                [-1.7128,  0.8221,  1.6303,  ..., -0.3516, -0.2165,  1.7750],\n",
       "                [-1.5248, -0.4612,  0.3135,  ..., -0.1600,  0.0551,  1.7049],\n",
       "                [-1.4136, -0.0453,  2.1713,  ..., -0.6869, -1.1165,  1.1987]]],\n",
       "              grad_fn=<AliasBackward0>)}"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#le modele est un transformer imbriqué dans un CNN\n",
    "#ici on peut voir le résultat a travers le CNN et le transformer\n",
    "ner_pipe.embedding.forward(mini_batch['ner']['embedding'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "15b0707d-df2f-4219-82e7-c9554b3f5626",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextCnnEncoder(\n",
       "  (embedding): Transformer(\n",
       "    (transformer): CamembertModel(\n",
       "      (embeddings): CamembertEmbeddings(\n",
       "        (word_embeddings): Embedding(32006, 768, padding_idx=1)\n",
       "        (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "        (token_type_embeddings): Embedding(1, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): CamembertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x CamembertLayer(\n",
       "            (attention): CamembertAttention(\n",
       "              (self): CamembertSdpaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): CamembertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): CamembertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): CamembertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pooler): CamembertPooler(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (module): TextCnn(\n",
       "    (convolutions): ModuleList(\n",
       "      (0): Conv1d(768, 768, kernel_size=(3,), stride=(1,))\n",
       "    )\n",
       "    (linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (residual): Residual()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_pipe.embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "38c16661-e91e-4785-bcb9-3189157d0e2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FoldedTensor([[[-1.8512,  0.9646,  0.5116,  ..., -1.1773,  0.7755,  0.5880],\n",
       "               [-0.6867,  1.3017,  0.9982,  ..., -1.1596, -1.0193,  1.3408],\n",
       "               [-2.0628,  1.0412,  0.2957,  ..., -1.5109, -1.6360,  1.6833],\n",
       "               ...,\n",
       "               [-1.5631,  0.6175,  1.1836,  ..., -0.8489, -0.7793,  1.6087],\n",
       "               [-2.5338, -0.7804,  0.6436,  ..., -0.7309,  0.1786,  1.6984],\n",
       "               [-1.8959, -0.1175,  1.5455,  ..., -0.5599, -1.2523,  1.1398]]],\n",
       "             grad_fn=<AliasBackward0>)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ici on peut voir le résultat a travers juste transformer\n",
    "ner_pipe.embedding.forward(mini_batch['ner']['embedding'])['embeddings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "75c2ae73-f63f-442c-b8f2-53bcb00501e6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 60])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#a travers le CNN\n",
    "result_cnn=ner_pipe.embedding.forward(mini_batch['ner']['embedding'])\n",
    "#a travers le module lineaire\n",
    "ner_pipe.linear.forward(result_cnn['embeddings']).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "b3b24d35-8ba8-4c5f-9e80-9b81b4cc839e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method forward in module edsnlp.pipes.trainable.layers.crf:\n",
      "\n",
      "forward(emissions, mask, target) method of edsnlp.pipes.trainable.layers.crf.MultiLabelBIOULDecoder instance\n",
      "    Compute the posterior reduced log-probabilities of the tags\n",
      "    given the emissions and the transition probabilities and\n",
      "    constraints of the CRF, ie the loss.\n",
      "\n",
      "\n",
      "    We could use the `propagate` method but this implementation\n",
      "    is faster.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    emissions: torch.FloatTensor\n",
      "        Shape: n_samples * n_tokens * ... * n_tags\n",
      "    mask: torch.BoolTensor\n",
      "        Shape: n_samples * n_tokens * ...\n",
      "    target: torch.BoolTensor\n",
      "        Shape: n_samples * n_tokens * ... * n_tags\n",
      "        The target tags represented with 1-hot encoding\n",
      "        We use 1-hot instead of long format to handle\n",
      "        cases when multiple tags at a given position are\n",
      "        allowed during training.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    torch.FloatTensor\n",
      "        Shape: ...\n",
      "        The loss\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#a travers le CNN\n",
    "result_cnn=ner_pipe.embedding.forward(mini_batch['ner']['embedding'])\n",
    "#a travers le module lineaire\n",
    "result_lin=ner_pipe.linear.forward(result_cnn['embeddings'])\n",
    "#a traverls le CRF BIOUL multilabel\n",
    "#il faut en plus les targets et les masks, que je ne connais pas\n",
    "help(ner_pipe.crf.forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c58e50d-0ec4-4a0e-9aef-82868cba7315",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4dcf97-57ae-4610-8c39-63121fe8ee34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10177b47-8fde-4f3c-9e0c-c07ed6f3b621",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kernel_local_python_312",
   "language": "python",
   "name": "kernel_local_python_312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
