{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Overview EDS-Pseudonymisation is a spaCy-based project used at APHP to extract and replace identifying entities in medical documents. Getting started EDS-Pseudonymisation is a spaCy project . We created a single workflow that: Converts the datasets to spaCy format Trains the pipeline Evaluates the pipeline using the test set Packages the resulting model to make it pip-installable To use it, you will need to supply: A labelled dataset A HuggingFace transformers model, or use camembert-base In any case, you will need to modify the configuration to reflect these changes. Entities Label Description ADRESSE Street address, eg 33 boulevard de Picpus DATE Any absolute date other than a birthdate DATE_NAISSANCE Birthdate HOPITAL Hospital name, eg H\u00f4pital Rothschild IPP Internal AP-HP identifier for patients, displayed as a number MAIL Email address NDA Internal AP-HP identifier for visits, displayed as a number NOM Any last name (patients, doctors, third parties) PRENOM Any first name (patients, doctors, etc) SECU Social security number TEL Any phone number VILLE Any city ZIP Any zip code Commands Command Description convert Convert the data to spaCy's binary format train Train the NER model evaluate Evaluate the model and export metrics package Package the trained model as a pip package visualize-model Visualize the model's output interactively using Streamlit Run the command with spacy project run [ command ] [ options ]","title":"Overview"},{"location":"#overview","text":"EDS-Pseudonymisation is a spaCy-based project used at APHP to extract and replace identifying entities in medical documents.","title":"Overview"},{"location":"#getting-started","text":"EDS-Pseudonymisation is a spaCy project . We created a single workflow that: Converts the datasets to spaCy format Trains the pipeline Evaluates the pipeline using the test set Packages the resulting model to make it pip-installable To use it, you will need to supply: A labelled dataset A HuggingFace transformers model, or use camembert-base In any case, you will need to modify the configuration to reflect these changes.","title":"Getting started"},{"location":"#entities","text":"Label Description ADRESSE Street address, eg 33 boulevard de Picpus DATE Any absolute date other than a birthdate DATE_NAISSANCE Birthdate HOPITAL Hospital name, eg H\u00f4pital Rothschild IPP Internal AP-HP identifier for patients, displayed as a number MAIL Email address NDA Internal AP-HP identifier for visits, displayed as a number NOM Any last name (patients, doctors, third parties) PRENOM Any first name (patients, doctors, etc) SECU Social security number TEL Any phone number VILLE Any city ZIP Any zip code","title":"Entities"},{"location":"#commands","text":"Command Description convert Convert the data to spaCy's binary format train Train the NER model evaluate Evaluate the model and export metrics package Package the trained model as a pip package visualize-model Visualize the model's output interactively using Streamlit Run the command with spacy project run [ command ] [ options ]","title":"Commands"},{"location":"changelog/","text":"Changelog v0.2.0 - 2023-05-04 Many fixes along the publication of our article : Tests for the rule-based components Code documentation and cleaning Experiment and analysis scripts Charts and tables in the Results page of our documentation v0.1.0 - 2022-05-13 Inception ! Features spaCy project for pseudonymisation Pseudonymisation-specific pipelines: pseudonymisation-rules for rule-based pseudonymisation pseudonymisation-dates for date detection and normalisation structured-data-matcher for structured data detection (eg first and last name, available in the information system) Evaluation methodology","title":"Changelog"},{"location":"changelog/#changelog","text":"","title":"Changelog"},{"location":"changelog/#v020-2023-05-04","text":"Many fixes along the publication of our article : Tests for the rule-based components Code documentation and cleaning Experiment and analysis scripts Charts and tables in the Results page of our documentation","title":"v0.2.0 - 2023-05-04"},{"location":"changelog/#v010-2022-05-13","text":"Inception !","title":"v0.1.0 - 2022-05-13"},{"location":"changelog/#features","text":"spaCy project for pseudonymisation Pseudonymisation-specific pipelines: pseudonymisation-rules for rule-based pseudonymisation pseudonymisation-dates for date detection and normalisation structured-data-matcher for structured data detection (eg first and last name, available in the information system) Evaluation methodology","title":"Features"},{"location":"dataset/","text":"Dataset Disclaimer We do not provide the dataset due to privacy and regulatory constraints. You will however find the description of the dataset below. We also release the code for the rule-based annotation system. Data Selection We annotated around 4000 documents, selected according to the distribution of AP-HP's Clinical Data Warehouse (CDW), to obtain a sample that is representative of the actual documents present within the CDW. Training data are selected among notes that were edited after August 2017, in order to skew the model towards more recent clinical notes. The test set, however, is sampled without any time constraints, to make sure the model performs well overall. To ensure the robustness of the model, training and test sets documents were generated from two different PDF extraction methods: the legacy method, based on PDFBox with a fixed mask our new method EDS-PDF with an adaptative (machine-learned) mask Annotated Entities We annotated clinical documents with the following entities : Label Description ADRESSE Street address, eg 33 boulevard de Picpus DATE Any absolute date other than a birthdate DATE_NAISSANCE Birthdate HOPITAL Hospital name, eg H\u00f4pital Rothschild IPP Internal AP-HP identifier for patients, displayed as a number MAIL Email address NDA Internal AP-HP identifier for visits, displayed as a number NOM Any last name (patients, doctors, third parties) PRENOM Any first name (patients, doctors, etc) SECU Social security number TEL Any phone number VILLE Any city ZIP Any zip code Statistics The following table presents the counts of annotated entities per split and per label. #T_09cec_ .col0 { border-left-style: solid; } #T_09cec_ .col2 { border-left-style: solid; } #T_09cec_ .col4 { border-left-style: solid; } train dev test edspdf pdfbox edspdf pdfbox edspdf pdfbox DATE 14711 2360 878 113 1973 2831 LASTNAME 4910 4299 292 236 625 4150 FIRSTNAME 3468 3826 215 216 478 3739 HOPITAL 1451 758 87 47 162 796 PHONE 397 1589 23 148 77 1851 BIRTHDATE 916 519 52 31 87 484 PATIENT ID 121 339 8 18 8 392 CITY 592 742 27 47 44 810 VISIT ID 49 283 7 17 6 282 ADDRESS 212 543 10 35 12 625 EMAIL 20 182 0 17 1 166 ZIP 215 552 10 35 14 635 NSS 73 79 6 7 4 32 ENTS 27135 16071 1615 967 3491 16793 DOCS 3025 348 200 22 348 348 Software The software tools used to annotate the documents with personal identification entities were: LabelStudio for the first annotation campaign Metanno for the second annotation campaign but any annotation software will do. The convert step takes as input either a jsonlines file ( .jsonl ) or a folder containing Standoff files ( .ann ) from an annotation with Brat . Feel free to submit a pull request if these formats do not suit you!","title":"Dataset"},{"location":"dataset/#dataset","text":"Disclaimer We do not provide the dataset due to privacy and regulatory constraints. You will however find the description of the dataset below. We also release the code for the rule-based annotation system.","title":"Dataset"},{"location":"dataset/#data-selection","text":"We annotated around 4000 documents, selected according to the distribution of AP-HP's Clinical Data Warehouse (CDW), to obtain a sample that is representative of the actual documents present within the CDW. Training data are selected among notes that were edited after August 2017, in order to skew the model towards more recent clinical notes. The test set, however, is sampled without any time constraints, to make sure the model performs well overall. To ensure the robustness of the model, training and test sets documents were generated from two different PDF extraction methods: the legacy method, based on PDFBox with a fixed mask our new method EDS-PDF with an adaptative (machine-learned) mask","title":"Data Selection"},{"location":"dataset/#annotated-entities","text":"We annotated clinical documents with the following entities : Label Description ADRESSE Street address, eg 33 boulevard de Picpus DATE Any absolute date other than a birthdate DATE_NAISSANCE Birthdate HOPITAL Hospital name, eg H\u00f4pital Rothschild IPP Internal AP-HP identifier for patients, displayed as a number MAIL Email address NDA Internal AP-HP identifier for visits, displayed as a number NOM Any last name (patients, doctors, third parties) PRENOM Any first name (patients, doctors, etc) SECU Social security number TEL Any phone number VILLE Any city ZIP Any zip code","title":"Annotated Entities"},{"location":"dataset/#statistics","text":"The following table presents the counts of annotated entities per split and per label. #T_09cec_ .col0 { border-left-style: solid; } #T_09cec_ .col2 { border-left-style: solid; } #T_09cec_ .col4 { border-left-style: solid; } train dev test edspdf pdfbox edspdf pdfbox edspdf pdfbox DATE 14711 2360 878 113 1973 2831 LASTNAME 4910 4299 292 236 625 4150 FIRSTNAME 3468 3826 215 216 478 3739 HOPITAL 1451 758 87 47 162 796 PHONE 397 1589 23 148 77 1851 BIRTHDATE 916 519 52 31 87 484 PATIENT ID 121 339 8 18 8 392 CITY 592 742 27 47 44 810 VISIT ID 49 283 7 17 6 282 ADDRESS 212 543 10 35 12 625 EMAIL 20 182 0 17 1 166 ZIP 215 552 10 35 14 635 NSS 73 79 6 7 4 32 ENTS 27135 16071 1615 967 3491 16793 DOCS 3025 348 200 22 348 348","title":"Statistics"},{"location":"dataset/#software","text":"The software tools used to annotate the documents with personal identification entities were: LabelStudio for the first annotation campaign Metanno for the second annotation campaign but any annotation software will do. The convert step takes as input either a jsonlines file ( .jsonl ) or a folder containing Standoff files ( .ann ) from an annotation with Brat . Feel free to submit a pull request if these formats do not suit you!","title":"Software"},{"location":"quickstart/","text":"Quickstart Deployment This project trains our pseudonymisation pipeline, and make it pip-installable. Requirements To use this repository, you will need to supply: A labelled dataset A HuggingFace transformers model, or use a publicly available model like camembert-base In any case, you will need to modify the configuration to reflect these changes. Installation Install the requirements by running the following command at the root of the repo poetry install Training a model EDS-Pseudonymisation is a spaCy project . We created a single workflow that: Converts the datasets to spaCy format Trains the pipeline Evaluates the pipeline using the test set Packages the resulting model to make it pip-installable To add a new dataset, run dvc import-url url/or/path/to/your/dataset data/dataset To (re-)train a model and package it, just run: dvc repro You should now be able to install and publish it: pip install dist/eds_pseudonymisation-0.2.0-* Use it To use it, execute import eds_pseudonymisation nlp = eds_pseudonymisation . load () doc = nlp ( \"\"\"En 1815, M. Charles-Fran\u00e7ois-Bienvenu Myriel \u00e9tait \u00e9v\u00eaque de Digne. C\u2019\u00e9tait un vieillard d\u2019environ soixante-quinze ans ; il occupait le si\u00e8ge de Digne depuis 1806. \"\"\" ) for ent in doc . ents : print ( ent , ent . label ) # 1815 DATE # Charles-Fran\u00e7ois-Bienvenu NOM # Myriel PRENOM # Digne VILLE # 1806 DATE","title":"Quickstart"},{"location":"quickstart/#quickstart","text":"","title":"Quickstart"},{"location":"quickstart/#deployment","text":"This project trains our pseudonymisation pipeline, and make it pip-installable.","title":"Deployment"},{"location":"quickstart/#requirements","text":"To use this repository, you will need to supply: A labelled dataset A HuggingFace transformers model, or use a publicly available model like camembert-base In any case, you will need to modify the configuration to reflect these changes.","title":"Requirements"},{"location":"quickstart/#installation","text":"Install the requirements by running the following command at the root of the repo poetry install","title":"Installation"},{"location":"quickstart/#training-a-model","text":"EDS-Pseudonymisation is a spaCy project . We created a single workflow that: Converts the datasets to spaCy format Trains the pipeline Evaluates the pipeline using the test set Packages the resulting model to make it pip-installable To add a new dataset, run dvc import-url url/or/path/to/your/dataset data/dataset To (re-)train a model and package it, just run: dvc repro You should now be able to install and publish it: pip install dist/eds_pseudonymisation-0.2.0-*","title":"Training a model"},{"location":"quickstart/#use-it","text":"To use it, execute import eds_pseudonymisation nlp = eds_pseudonymisation . load () doc = nlp ( \"\"\"En 1815, M. Charles-Fran\u00e7ois-Bienvenu Myriel \u00e9tait \u00e9v\u00eaque de Digne. C\u2019\u00e9tait un vieillard d\u2019environ soixante-quinze ans ; il occupait le si\u00e8ge de Digne depuis 1806. \"\"\" ) for ent in doc . ents : print ( ent , ent . label ) # 1815 DATE # Charles-Fran\u00e7ois-Bienvenu NOM # Myriel PRENOM # Digne VILLE # 1806 DATE","title":"Use it"},{"location":"reproducibility/","text":"Reproducibility To guarantee the reproducibility of our models, we rely on virtual environments and VCS tools. Environments We use Poetry to validate the constraints generated by the dependencies of our model, and lock the versions that were used to generate a model, in a poetry.lock file. This file can be reused to reinstall a previous environment by running poetry install --with docs Versioning We use DVC to version the experiences, models and datasets used. To add and version a new dataset, run dvc import-url url/or/path/to/your/dataset data/dataset To (re-)train a model and package it, just run: dvc repro For more information about DVC, make sure to visit their documentation . Article experiments To reproduce the results of our article, run the experiments.py script to queue and run all the experiments with DVC: $ python scripts/experiments.py $ dvc exp run --queue --run-all Tip for Slurm environments If your computing resources are managed with Slurm, you can run dvc exp queue-worker from Slurm jobs instead of the last command to parallelize the experiments across multiple nodes. my_slurm_job.sh # SBATCH ... dvc exp queue-worker dvc-worker- $SLURM_JOB_ID -v $ sbatch my_slurm_job.sh # first job $ sbatch my_slurm_job.sh # launch as many jobs at once as needed To reproduce (some) of the figures of our article, run the analysis.py script to generate the charts and tables in the docs/assets/figures folder. $ python scripts/analysis.py INFO:root:Loading experiments INFO:root:Found 110 experiments INFO:root:Building corpus statistics table INFO:root:Computing results table, this can take a while ... INFO:root:Plotting BERT ablation experiments INFO:root:Plotting results by labels INFO:root:Plotting document type ablation experiments INFO:root:Building comparison table of PDF extraction methods INFO:root:Building comparison table of ML vs rule-based and visualize them by serving the documentation $ mkdocs serve","title":"Reproducibility"},{"location":"reproducibility/#reproducibility","text":"To guarantee the reproducibility of our models, we rely on virtual environments and VCS tools.","title":"Reproducibility"},{"location":"reproducibility/#environments","text":"We use Poetry to validate the constraints generated by the dependencies of our model, and lock the versions that were used to generate a model, in a poetry.lock file. This file can be reused to reinstall a previous environment by running poetry install --with docs","title":"Environments"},{"location":"reproducibility/#versioning","text":"We use DVC to version the experiences, models and datasets used. To add and version a new dataset, run dvc import-url url/or/path/to/your/dataset data/dataset To (re-)train a model and package it, just run: dvc repro For more information about DVC, make sure to visit their documentation .","title":"Versioning"},{"location":"reproducibility/#article-experiments","text":"To reproduce the results of our article, run the experiments.py script to queue and run all the experiments with DVC: $ python scripts/experiments.py $ dvc exp run --queue --run-all Tip for Slurm environments If your computing resources are managed with Slurm, you can run dvc exp queue-worker from Slurm jobs instead of the last command to parallelize the experiments across multiple nodes. my_slurm_job.sh # SBATCH ... dvc exp queue-worker dvc-worker- $SLURM_JOB_ID -v $ sbatch my_slurm_job.sh # first job $ sbatch my_slurm_job.sh # launch as many jobs at once as needed To reproduce (some) of the figures of our article, run the analysis.py script to generate the charts and tables in the docs/assets/figures folder. $ python scripts/analysis.py INFO:root:Loading experiments INFO:root:Found 110 experiments INFO:root:Building corpus statistics table INFO:root:Computing results table, this can take a while ... INFO:root:Plotting BERT ablation experiments INFO:root:Plotting results by labels INFO:root:Plotting document type ablation experiments INFO:root:Building comparison table of PDF extraction methods INFO:root:Building comparison table of ML vs rule-based and visualize them by serving the documentation $ mkdocs serve","title":"Article experiments"},{"location":"results/","text":"Results Our article is available on arXiv . You will find below some of the results presented in the article, as well as interactive charts. General results #T_59e38_ .col1 { border-left-style: solid; } #T_59e38_ .col4 { border-left-style: solid; } #T_59e38_ .col7 { border-left-style: solid; } #T_59e38_ .col10 { border-left-style: solid; } #T_59e38_ .col13 { border-left-style: solid; } Label Precision Recall F1 Redact Full redact RB ML Hybrid RB ML Hybrid RB ML Hybrid RB ML Hybrid RB ML Hybrid ADDRESS 99.5 99.7 99.4 73.5 93.6 93.8 84.3 96.4 96.3 74.9 94.2 94.3 83.9 98.0 98.4 BIRTHDATE 97.8 98.1 98.2 78.2 98.5 98.5 86.6 98.3 98.3 98.5 99.8 99.8 98.7 99.7 99.7 CITY 94.9 97.5 97.3 41.7 96.4 96.5 57.3 96.9 96.9 41.7 96.5 96.5 61.4 98.1 98.2 DATE 93.9 99.7 99.7 95.8 99.3 99.3 94.9 99.5 99.5 96.1 99.6 99.6 76.7 95.4 95.4 EMAIL 99.9 79.4 90.5 96.8 66.3 100.0 98.3 66.2 93.9 96.8 66.3 100.0 98.7 99.6 99.9 FIRSTNAME 96.8 98.5 98.5 39.1 97.6 97.6 55.6 98.0 98.0 45.1 98.8 98.9 46.7 97.4 97.4 LASTNAME 89.8 98.3 98.3 59.3 98.1 98.6 71.4 98.2 98.4 59.8 99.1 99.6 47.3 96.4 97.2 NSS 98.1 83.9 83.7 96.6 97.8 99.2 97.3 89.8 90.3 96.6 99.4 100.0 99.7 99.9 100.0 PATIENT ID 99.6 99.3 99.3 89.7 95.9 95.9 94.3 97.5 97.5 89.7 98.8 98.8 93.1 99.1 99.1 PHONE 99.9 99.7 99.7 93.9 99.6 99.6 96.8 99.7 99.7 93.9 99.6 99.6 93.2 98.8 99.0 VISIT ID 98.8 87.3 87.3 76.9 81.1 81.3 85.5 83.7 83.9 77.1 81.8 82.0 97.0 98.1 98.3 ZIP 100.0 99.5 99.5 80.8 98.7 99.5 89.4 99.1 99.5 80.9 98.7 99.5 87.4 99.3 99.9 ALL 95.8 99.1 99.1 82.7 98.8 98.9 88.7 99.0 99.0 84.8 99.3 99.4 31.9 84.4 86.2 If you have trouble seeing the chart, please refresh the page. { \"schema-url\": \"../assets/figures/label_scores.json\" } Impact of the language model #T_7b589_ .col1 { border-left-style: solid; } #T_7b589_ .col4 { border-left-style: solid; } #T_7b589_ .col7 { border-left-style: solid; } #T_7b589_ .col10 { border-left-style: solid; } #T_7b589_ .col13 { border-left-style: solid; } Label Precision Recall F1 Redact Full redact RB ML Hybrid RB ML Hybrid RB ML Hybrid RB ML Hybrid RB ML Hybrid ADDRESS 99.5 99.7 99.4 73.5 93.6 93.8 84.3 96.4 96.3 74.9 94.2 94.3 83.9 98.0 98.4 BIRTHDATE 97.8 98.1 98.2 78.2 98.5 98.5 86.6 98.3 98.3 98.5 99.8 99.8 98.7 99.7 99.7 CITY 94.9 97.5 97.3 41.7 96.4 96.5 57.3 96.9 96.9 41.7 96.5 96.5 61.4 98.1 98.2 DATE 93.9 99.7 99.7 95.8 99.3 99.3 94.9 99.5 99.5 96.1 99.6 99.6 76.7 95.4 95.4 EMAIL 99.9 79.4 90.5 96.8 66.3 100.0 98.3 66.2 93.9 96.8 66.3 100.0 98.7 99.6 99.9 FIRSTNAME 96.8 98.5 98.5 39.1 97.6 97.6 55.6 98.0 98.0 45.1 98.8 98.9 46.7 97.4 97.4 LASTNAME 89.8 98.3 98.3 59.3 98.1 98.6 71.4 98.2 98.4 59.8 99.1 99.6 47.3 96.4 97.2 NSS 98.1 83.9 83.7 96.6 97.8 99.2 97.3 89.8 90.3 96.6 99.4 100.0 99.7 99.9 100.0 PATIENT ID 99.6 99.3 99.3 89.7 95.9 95.9 94.3 97.5 97.5 89.7 98.8 98.8 93.1 99.1 99.1 PHONE 99.9 99.7 99.7 93.9 99.6 99.6 96.8 99.7 99.7 93.9 99.6 99.6 93.2 98.8 99.0 VISIT ID 98.8 87.3 87.3 76.9 81.1 81.3 85.5 83.7 83.9 77.1 81.8 82.0 97.0 98.1 98.3 ZIP 100.0 99.5 99.5 80.8 98.7 99.5 89.4 99.1 99.5 80.9 98.7 99.5 87.4 99.3 99.9 ALL 95.8 99.1 99.1 82.7 98.8 98.9 88.7 99.0 99.0 84.8 99.3 99.4 31.9 84.4 86.2 If you have trouble seeing the chart, please refresh the page. { \"schema-url\": \"../assets/figures/bert_ablation.json\" } Impact of the PDF extraction step PDF extractor P R F1 Redact Full redact edspdf 99.1 \u00b1 0.1 98.8 \u00b1 0.1 98.9 \u00b1 0.1 99.2 \u00b1 0.1 93.1 \u00b1 1.0 pdfbox 99.1 \u00b1 0.0 98.9 \u00b1 0.2 99.0 \u00b1 0.1 99.4 \u00b1 0.1 75.7 \u00b1 3.0 Impact of the number of training examples If you have trouble seeing the chart, please refresh the page. { \"schema-url\": \"../assets/figures/limit_ablation.json\" } Impact of the missing document types If you have trouble seeing the chart, please refresh the page. { \"schema-url\": \"../assets/figures/doc_type_ablation.json\" }","title":"Results"},{"location":"results/#results","text":"Our article is available on arXiv . You will find below some of the results presented in the article, as well as interactive charts.","title":"Results"},{"location":"results/#general-results","text":"#T_59e38_ .col1 { border-left-style: solid; } #T_59e38_ .col4 { border-left-style: solid; } #T_59e38_ .col7 { border-left-style: solid; } #T_59e38_ .col10 { border-left-style: solid; } #T_59e38_ .col13 { border-left-style: solid; } Label Precision Recall F1 Redact Full redact RB ML Hybrid RB ML Hybrid RB ML Hybrid RB ML Hybrid RB ML Hybrid ADDRESS 99.5 99.7 99.4 73.5 93.6 93.8 84.3 96.4 96.3 74.9 94.2 94.3 83.9 98.0 98.4 BIRTHDATE 97.8 98.1 98.2 78.2 98.5 98.5 86.6 98.3 98.3 98.5 99.8 99.8 98.7 99.7 99.7 CITY 94.9 97.5 97.3 41.7 96.4 96.5 57.3 96.9 96.9 41.7 96.5 96.5 61.4 98.1 98.2 DATE 93.9 99.7 99.7 95.8 99.3 99.3 94.9 99.5 99.5 96.1 99.6 99.6 76.7 95.4 95.4 EMAIL 99.9 79.4 90.5 96.8 66.3 100.0 98.3 66.2 93.9 96.8 66.3 100.0 98.7 99.6 99.9 FIRSTNAME 96.8 98.5 98.5 39.1 97.6 97.6 55.6 98.0 98.0 45.1 98.8 98.9 46.7 97.4 97.4 LASTNAME 89.8 98.3 98.3 59.3 98.1 98.6 71.4 98.2 98.4 59.8 99.1 99.6 47.3 96.4 97.2 NSS 98.1 83.9 83.7 96.6 97.8 99.2 97.3 89.8 90.3 96.6 99.4 100.0 99.7 99.9 100.0 PATIENT ID 99.6 99.3 99.3 89.7 95.9 95.9 94.3 97.5 97.5 89.7 98.8 98.8 93.1 99.1 99.1 PHONE 99.9 99.7 99.7 93.9 99.6 99.6 96.8 99.7 99.7 93.9 99.6 99.6 93.2 98.8 99.0 VISIT ID 98.8 87.3 87.3 76.9 81.1 81.3 85.5 83.7 83.9 77.1 81.8 82.0 97.0 98.1 98.3 ZIP 100.0 99.5 99.5 80.8 98.7 99.5 89.4 99.1 99.5 80.9 98.7 99.5 87.4 99.3 99.9 ALL 95.8 99.1 99.1 82.7 98.8 98.9 88.7 99.0 99.0 84.8 99.3 99.4 31.9 84.4 86.2 If you have trouble seeing the chart, please refresh the page. { \"schema-url\": \"../assets/figures/label_scores.json\" }","title":"General results"},{"location":"results/#impact-of-the-language-model","text":"#T_7b589_ .col1 { border-left-style: solid; } #T_7b589_ .col4 { border-left-style: solid; } #T_7b589_ .col7 { border-left-style: solid; } #T_7b589_ .col10 { border-left-style: solid; } #T_7b589_ .col13 { border-left-style: solid; } Label Precision Recall F1 Redact Full redact RB ML Hybrid RB ML Hybrid RB ML Hybrid RB ML Hybrid RB ML Hybrid ADDRESS 99.5 99.7 99.4 73.5 93.6 93.8 84.3 96.4 96.3 74.9 94.2 94.3 83.9 98.0 98.4 BIRTHDATE 97.8 98.1 98.2 78.2 98.5 98.5 86.6 98.3 98.3 98.5 99.8 99.8 98.7 99.7 99.7 CITY 94.9 97.5 97.3 41.7 96.4 96.5 57.3 96.9 96.9 41.7 96.5 96.5 61.4 98.1 98.2 DATE 93.9 99.7 99.7 95.8 99.3 99.3 94.9 99.5 99.5 96.1 99.6 99.6 76.7 95.4 95.4 EMAIL 99.9 79.4 90.5 96.8 66.3 100.0 98.3 66.2 93.9 96.8 66.3 100.0 98.7 99.6 99.9 FIRSTNAME 96.8 98.5 98.5 39.1 97.6 97.6 55.6 98.0 98.0 45.1 98.8 98.9 46.7 97.4 97.4 LASTNAME 89.8 98.3 98.3 59.3 98.1 98.6 71.4 98.2 98.4 59.8 99.1 99.6 47.3 96.4 97.2 NSS 98.1 83.9 83.7 96.6 97.8 99.2 97.3 89.8 90.3 96.6 99.4 100.0 99.7 99.9 100.0 PATIENT ID 99.6 99.3 99.3 89.7 95.9 95.9 94.3 97.5 97.5 89.7 98.8 98.8 93.1 99.1 99.1 PHONE 99.9 99.7 99.7 93.9 99.6 99.6 96.8 99.7 99.7 93.9 99.6 99.6 93.2 98.8 99.0 VISIT ID 98.8 87.3 87.3 76.9 81.1 81.3 85.5 83.7 83.9 77.1 81.8 82.0 97.0 98.1 98.3 ZIP 100.0 99.5 99.5 80.8 98.7 99.5 89.4 99.1 99.5 80.9 98.7 99.5 87.4 99.3 99.9 ALL 95.8 99.1 99.1 82.7 98.8 98.9 88.7 99.0 99.0 84.8 99.3 99.4 31.9 84.4 86.2 If you have trouble seeing the chart, please refresh the page. { \"schema-url\": \"../assets/figures/bert_ablation.json\" }","title":"Impact of the language model"},{"location":"results/#impact-of-the-pdf-extraction-step","text":"PDF extractor P R F1 Redact Full redact edspdf 99.1 \u00b1 0.1 98.8 \u00b1 0.1 98.9 \u00b1 0.1 99.2 \u00b1 0.1 93.1 \u00b1 1.0 pdfbox 99.1 \u00b1 0.0 98.9 \u00b1 0.2 99.0 \u00b1 0.1 99.4 \u00b1 0.1 75.7 \u00b1 3.0","title":"Impact of the PDF extraction step"},{"location":"results/#impact-of-the-number-of-training-examples","text":"If you have trouble seeing the chart, please refresh the page. { \"schema-url\": \"../assets/figures/limit_ablation.json\" }","title":"Impact of the number of training examples"},{"location":"results/#impact-of-the-missing-document-types","text":"If you have trouble seeing the chart, please refresh the page. { \"schema-url\": \"../assets/figures/doc_type_ablation.json\" }","title":"Impact of the missing document types"}]}