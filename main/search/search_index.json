{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":"<p>EDS-Pseudo is a project aimed at detecting identifying entities in textual documents, and was primarily tested on clinical reports at AP-HP's Clinical Data Warehouse (EDS).</p> <p>The model is built on top of edsnlp, and consists in a hybrid model (rule-based + deep learning) for which we provide rules <code>eds_pseudo/pipes</code> and a training recipe <code>scripts/train.py</code>.</p> <p>We also provide a small set of fictive documents <code>data/gen_dataset/train.jsonl</code> to test the method.</p> <p>The entities that are detected are listed below.</p> Label Description <code>ADRESSE</code> Street address, eg <code>33 boulevard de Picpus</code> <code>DATE</code> Any absolute date other than a birthdate <code>DATE_NAISSANCE</code> Birthdate <code>HOPITAL</code> Hospital name, eg <code>H\u00f4pital Rothschild</code> <code>IPP</code> Internal AP-HP identifier for patients, displayed as a number <code>MAIL</code> Email address <code>NDA</code> Internal AP-HP identifier for visits, displayed as a number <code>NOM</code> Any last name (patients, doctors, third parties) <code>PRENOM</code> Any first name (patients, doctors, etc) <code>SECU</code> Social security number <code>TEL</code> Any phone number <code>VILLE</code> Any city <code>ZIP</code> Any zip code"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#v030-2023-12-01","title":"v0.3.0 - 2023-12-01","text":"<ul> <li>Refactoring and fixes to use edsnlp instead of spaCy.</li> <li>Renamed <code>eds_pseudonymisation</code> to <code>eds_pseudo</code> and default model name to <code>eds_pseudo_aphp</code>.</li> <li>Renamed <code>pipelines</code> to <code>pipes</code></li> <li>New <code>scripts/train.py</code> script to train the model</li> </ul>"},{"location":"changelog/#v020-unreleased","title":"## v0.2.0 - Unreleased","text":"<p>Some fixes to enable training the model: - committed the missing script <code>infer.py</code> - changed config default bert model to <code>camembert-base</code> - put <code>config.cfg</code> as a dependency, not params - default to cpu training - allow for missing metadata (i.e. omop's <code>note_class_source_value</code>)</p>"},{"location":"changelog/#v020-2023-05-04","title":"v0.2.0 - 2023-05-04","text":"<p>Many fixes along the publication of our article:</p> <ul> <li>Tests for the rule-based components</li> <li>Code documentation and cleaning</li> <li>Experiment and analysis scripts</li> <li>Charts and tables in the Results page of our documentation</li> </ul>"},{"location":"changelog/#v010-2022-05-13","title":"v0.1.0 - 2022-05-13","text":"<p>Inception ! </p>"},{"location":"changelog/#features","title":"Features","text":"<ul> <li>spaCy project for pseudonymisation</li> <li>Pseudonymisation-specific pipelines:<ul> <li><code>pseudonymisation-rules</code> for rule-based pseudonymisation</li> <li><code>pseudonymisation-dates</code> for date detection and normalisation</li> <li><code>structured-data-matcher</code> for structured data detection (eg first and last name, available in the information system)</li> </ul> </li> <li>Evaluation methodology</li> </ul>"},{"location":"dataset/","title":"Dataset","text":"<p>Disclaimer</p> <p>We do not provide the dataset due to privacy and regulatory constraints. You will however find the description of the dataset below. We also release the code for the rule-based annotation system.</p> <p>You can find fictive data in the <code>data/gen_dataset</code> folder to test the model.</p>"},{"location":"dataset/#format","title":"Format","text":"<p>By default, we expect the annotations to follow the format of the demo dataset data/gen_dataset, but you can change the format by modifying the config file, and the \"Datasets\" part of it in particular, or the code of the adapter.</p>"},{"location":"dataset/#data-selection","title":"Data Selection","text":"<p>We annotated around 4000 documents, selected according to the distribution of AP-HP's Clinical Data Warehouse (CDW), to obtain a sample that is representative of the actual documents present within the CDW.</p> <p>Training data are selected among notes that were edited after August 2017, in order to skew the model towards more recent clinical notes. The test set, however, is sampled without any time constraints, to make sure the model performs well overall.</p> <p>To ensure the robustness of the model, training and test sets documents were generated from two different PDF extraction methods:</p> <ul> <li>the legacy method, based on PDFBox with a fixed mask</li> <li>our new method EDS-PDF with an adaptative   (machine-learned) mask</li> </ul>"},{"location":"dataset/#annotated-entities","title":"Annotated Entities","text":"<p>We annotated clinical documents with the following entities :</p> Label Description <code>ADRESSE</code> Street address, eg <code>33 boulevard de Picpus</code> <code>DATE</code> Any absolute date other than a birthdate <code>DATE_NAISSANCE</code> Birthdate <code>HOPITAL</code> Hospital name, eg <code>H\u00f4pital Rothschild</code> <code>IPP</code> Internal AP-HP identifier for patients, displayed as a number <code>MAIL</code> Email address <code>NDA</code> Internal AP-HP identifier for visits, displayed as a number <code>NOM</code> Any last name (patients, doctors, third parties) <code>PRENOM</code> Any first name (patients, doctors, etc) <code>SECU</code> Social security number <code>TEL</code> Any phone number <code>VILLE</code> Any city <code>ZIP</code> Any zip code"},{"location":"dataset/#statistics","title":"Statistics","text":"<p>To inspect the statistics for the latest version of our dataset, please refer to the latest release.</p>"},{"location":"dataset/#software","title":"Software","text":"<p>The software tools used to annotate the documents with personal identification entities were:</p> <ul> <li>LabelStudio for the first annotation campaign</li> <li>Metanno for the second annotation campaign but any annotation software will do.</li> </ul>"},{"location":"inference/","title":"Inference","text":""},{"location":"inference/#parallelizing-inference","title":"Parallelizing inference","text":"<p>When processing multiple documents, we can optimize the inference by parallelizing the computation on multiple cores and GPUs.</p> <p>Assuming we have parquet files that follow the schema described in the quickstart page, we can use the following code to parallelize the processing of the documents. For a deep learning model, the idea is to have enough CPU workers, which manage the IO, rule-based components and torch pre- / post-processing, to pre-process batches of documents in parallel to the deep learning model computation in order to maximise GPU utilisation.</p> <p>We will output the result to a dataset parquet folder, with 8192 documents per file. This dataset can be located on the filesystem, or on a distributed filesystem like HDFS or S3.</p> <pre><code>import edsnlp\n\ndata = edsnlp.data.read_parquet(\"path/to/parquet_folder\", converter=converter)\ndata = data.map_pipeline(nlp)\ndata = data.set_processing(\n    # 1 GPUs to accelerate deep-learning pipes\n    num_gpu_workers=1,  # or more if you have more GPUs\n    # 4 CPUs for rb pipes, IO and pre- / post-processing\n    num_cpu_workers=4,\n    # Each worker will process 32 docs at a time\n    batch_size=32,\n    # Track the progress of the processing\n    show_progress=True,\n)\ndata.write_parquet(\n    \"hdfs://path/to/output_folder\",\n    converter=\"ents\",\n    # Each file will contain the annotations of 8192 docs\n    num_rows_per_file=8192,\n    # Each worker will write directly to the output folder\n    # without\n    write_in_worker=True,\n)\n</code></pre> <p>All the parameters of the <code>set_processing</code> method can be found in the API documentation.</p> <p>Below are some tips to help you choose the right number of workers and batch size. You should multiply the number of CPU workers <code>num_cpu_workers</code> by the number of GPUs to get the total number of CPU workers. These numbers are only indicative, and you should take into account:</p> <ul> <li>the average size of your documents (bigger texts means smaller batch size), these numbers   are for documents averaging 2200 characters</li> <li>number of available cores: ensure that <code>num_cpu_workers</code> + <code>num_gpu_workers</code> &lt; number of cores</li> <li>the number of rule-based pipes : if you have a lot of rule-based pipes, you should   increase the number of CPU workers. These numbers assume that you have disabled the   <code>dates</code> and <code>simpler_rules/PERSON</code> patterns.</li> </ul> <p>Make sure to monitor the GPU usage with <code>nvidia-smi</code> or <code>watch -n 5 nvidia-smi</code> to make sure your GPU(s) are fully utilized.</p> GPU model <code>batch_size</code> <code>num_cpu_workers</code> per gpu ~80Gb A100 &lt; 128 6-8 ~32Gb V100 &lt; 48 3-4 ~24Gb P40 &lt; 32 1-2"},{"location":"inference/#model-quantization","title":"Model quantization","text":"<p>If you are using a huggingface transformer-based model, you can use quantization to reduce the memory footprint of the model. This will both reduce the memory usage of the model and speed up inference. To use it, simply add the following config overrides when loading the model:</p> Loading the packaged modelLoading from the folder <pre><code>import eds_pseudo_aphp\n\n# Load the model\nnlp = eds_pseudo_aphp.load(\n    {\n        \"components\": {\n            \"ner\": {\n                \"embedding\": {\n                    \"embedding\": {\n                        \"quantization\": {\n                            \"load_in_4bit\": True,\n                            \"bnb_4bit_compute_dtype\": \"float16\",\n                        }\n                    }\n                }\n            }\n        }\n    }\n)\n</code></pre> <pre><code>import edsnlp\n\n# Load the model\nnlp = edsnlp.load(\n    \"artifacts/model-last\",\n    {\n        \"components\": {\n            \"ner\": {\n                \"embedding\": {\n                    \"embedding\": {\n                        \"quantization\": {\n                            \"load_in_4bit\": True,\n                            \"bnb_4bit_compute_dtype\": \"float16\",\n                        }\n                    }\n                }\n            }\n        }\n    },\n)\n</code></pre>"},{"location":"quickstart/","title":"Quickstart","text":""},{"location":"quickstart/#installation","title":"Installation","text":"<p>First, clone the repository</p> <pre><code>git clone https://github.com/aphp/eds-pseudo.git\ncd eds-pseudo\n</code></pre> <p>And install the dependencies:</p> <pre><code>poetry install\n</code></pre> <p>If you face issues with the installation, try to lower the maximum python version to &lt;= 3.10 (in <code>pyproject.toml</code>).</p>"},{"location":"quickstart/#without-machine-learning","title":"Without machine learning","text":"<p>If you do not have a labelled dataset, you can still use the rule-based components of the model.</p> <pre><code>import edsnlp\n\nnlp = edsnlp.blank(\"eds\")\n\n# Some text cleaning\nnlp.add_pipe(\"eds.normalizer\")\n\n# Various simple rules\nnlp.add_pipe(\n    \"eds_pseudo.simple_rules\",\n    config={\"pattern_keys\": [\"TEL\", \"MAIL\", \"SECU\", \"PERSON\"]},\n)\n\n# Address detection\nnlp.add_pipe(\"eds_pseudo.addresses\")\n\n# Date detection\nnlp.add_pipe(\"eds_pseudo.dates\")\n\n# Contextual rules (requires a dict of info about the patient)\nnlp.add_pipe(\"eds_pseudo.context\")\n\n# Apply it to a text\ndoc = nlp(\n    \"En 2015, M. Charles-Fran\u00e7ois-Bienvenu \"  # (1)!\n    \"Myriel \u00e9tait \u00e9v\u00eaque de Digne. C\u2019\u00e9tait un vieillard \"\n    \"d\u2019environ soixante-quinze ans ; il occupait le \"\n    \"si\u00e8ge de Digne depuis 2006.\"\n)\n\nfor ent in doc.ents:\n    print(ent, ent.label_)\n\n# 2015 DATE\n# Charles-Fran\u00e7ois-Bienvenu NOM\n# Myriel PRENOM\n# 2006 DATE\n</code></pre> <ol> <li>The original date is 1815, but the rule-based date detection only matches dates after    1900 to avoid false positives.</li> </ol> <p>You can observe that the model is not flawless : \"Digne\" is not detected as a city. This can be alleviated by adding contextual information about the patient (see below), or by training a model.</p>"},{"location":"quickstart/#apply-on-multiple-documents","title":"Apply on multiple documents","text":"<p>We recommend you check out the edsnlp's tutorial on how to process multiple documents.</p> <p>Assuming we have a dataframe <code>df</code> with columns <code>note_id</code>, <code>text</code> and an optional column <code>context</code>, containing information about the patient, e.g.:</p> note_id text context doc-1 En 2015, M. Charles-Fran\u00e7ois-Bienvenu ... {\"VILLE\": \"DIGNE\", \"zip\": \"04070\"} doc-2 Mme. Ange-Gardien Josephine est admise pour irritation des tendons fl\u00e9chisseurs doc-3 josephine.ange-gardien @ test.com <p>We can apply the model to all the documents with the following code:</p> <pre><code>import edsnlp\n\n\n# Function to convert a row of the dataframe to a Doc object\ndef converter(row):\n    tokenizer = edsnlp.data.converters.get_current_tokenizer()\n    doc = tokenizer(row[\"text\"])\n    doc._.note_id = row[\"note_id\"]\n    ctx = row[\"context\"]\n    if isinstance(ctx, dict):\n        doc._.context = {k: v if isinstance(v, list) else [v] for k, v in ctx.items()}\n    return doc\n\n\ndata = edsnlp.data.from_pandas(df, converter=converter)\ndata = data.map_pipeline(nlp)\ndata.to_pandas(converter=\"ents\")\n</code></pre> <p>and we get the following dataframe:</p> note_id start end label lexical_variant doc-1 3 7 DATE 2015 doc-1 12 37 NOM Charles-Fran\u00e7ois-Bienvenu doc-1 38 44 PRENOM Myriel doc-1 61 66 VILLE Digne doc-1 145 150 VILLE Digne doc-1 158 162 DATE 2006 doc-2 5 17 NOM Ange-Gardien doc-2 18 27 PRENOM Jos\u00e9phine doc-3 0 33 MAIL josephine.ange-gardien @ test.com"},{"location":"reproducibility/","title":"Reproducibility","text":"<p>To guarantee the reproducibility of our models, we rely on virtual environments and VCS tools.</p>"},{"location":"reproducibility/#environments","title":"Environments","text":"<p>We use Poetry to validate the constraints generated by the dependencies of our model, and lock the versions that were used to generate a model, in a <code>poetry.lock</code> file.</p> <p>This file can be reused to reinstall a previous environment by running</p> <pre><code>poetry install --with docs\n</code></pre>"},{"location":"reproducibility/#versioning","title":"Versioning","text":"<p>We use DVC to version the experiences, models and datasets used.</p> <p>To add and version a new dataset, run</p> <pre><code>dvc import-url url/or/path/to/your/dataset data/dataset\n</code></pre> <p>To (re-)train a model and package it, just run:</p> <pre><code>dvc repro\n</code></pre> <p>For more information about DVC, make sure to visit their documentation.</p>"},{"location":"reproducibility/#article-experiments","title":"Article experiments","text":"<p>To reproduce the results of our article, run the <code>experiments.py</code> script to queue and run all the experiments with DVC:</p> <pre><code>python scripts/experiments.py\ndvc exp run --queue --run-all\n</code></pre> Tip for Slurm environments <p>If your computing resources are managed with Slurm, you can run <code>dvc exp queue-worker</code> from Slurm jobs instead of the last command to parallelize the experiments across multiple nodes.</p> my_slurm_job.sh<pre><code># SBATCH ...\n\ndvc exp queue-worker dvc-worker-$SLURM_JOB_ID -v\n</code></pre> <pre><code>$ sbatch my_slurm_job.sh  # first job\n$ sbatch my_slurm_job.sh  # launch as many jobs at once as needed\n</code></pre> <p>To reproduce (some) of the figures of our article, run the <code>analysis.py</code> script to generate the charts and tables in the <code>docs/assets/figures</code> folder.</p> <pre><code>python scripts/analysis.py\n</code></pre> <pre><code>INFO:root:Loading experiments\nINFO:root:Found 110 experiments\nINFO:root:Building corpus statistics table\nINFO:root:Computing results table, this can take a while...\nINFO:root:Plotting BERT ablation experiments\nINFO:root:Plotting results by labels\nINFO:root:Plotting document type ablation experiments\nINFO:root:Building comparison table of PDF extraction methods\nINFO:root:Building comparison table of ML vs rule-based\n</code></pre> <p>and visualize them by serving the documentation</p> <pre><code>mkdocs serve\n</code></pre>"},{"location":"results/","title":"Results","text":"<p>Our article is available on arXiv.</p> <p>To inspect the results for the latest version of our system, please refer to the latest release page.</p>"},{"location":"training/","title":"Training","text":""},{"location":"training/#requirements","title":"Requirements","text":"<p>To train a model, you will need to provide:</p> <ul> <li>A labelled dataset</li> <li>A HuggingFace transformers model, or a publicly available model like <code>camembert-base</code></li> <li>Ideally, a GPU to accelerate training</li> </ul> <p>In any case, you will need to modify the configs/config.cfg file to reflect these changes. This configuration already contains the rule-based components of the previous section, feel free to add or remove them as you see fit. You may also want to modify the pyproject.toml file to change the name of packaged model (defaults to <code>eds-pseudo-aphp</code>).</p>"},{"location":"training/#dvc","title":"DVC","text":"<p>We use DVC to manage the training pipeline. DVC is a version control system for data science and machine learning projects. We recommend you use it too. First, import some data (this basically copies the data to <code>data/dataset</code>, but in a version-controlled fashion):</p> <pre><code>dvc import-url url/or/path/to/your/dataset data/dataset\n</code></pre> <p>and execute the following command to (re)train the model and package it</p> <pre><code>dvc repro\n</code></pre> Content of the <code>dvc.yaml</code> file <p>The above command runs the <code>dvc.yaml</code> config file to sequentially execute :</p> <pre><code># Trains the model, and outputs it to artifacts/model-last\npython scripts/train.py --config configs/config.cfg\n\n# Evaluates the model, and outputs the results to artifacts\npython scripts/evaluate.py --config configs/config.cfg\n\n# Packages the model\npython scripts/package.py\n</code></pre> <p>You should now be able to install and publish it:</p> <pre><code>pip install dist/eds_pseudo_aphp-0.3.0-*\n</code></pre>"},{"location":"training/#use-it","title":"Use it","text":"<p>To test it, execute</p> Loading the packaged modelLoading from the folder <pre><code>import eds_pseudo_aphp\n\n# Load the model\nnlp = eds_pseudo_aphp.load()\n</code></pre> <pre><code>import edsnlp\n\n# Load the model\nnlp = edsnlp.load(\"artifacts/model-last\")\n</code></pre> <pre><code># Apply it to a text\ndoc = nlp(\n    \"En 1815, M. Charles-Fran\u00e7ois-Bienvenu \"\n    \"Myriel \u00e9tait \u00e9v\u00eaque de Digne. C\u2019\u00e9tait un vieillard \"\n    \"d\u2019environ soixante-quinze ans ; il occupait le \"\n    \"si\u00e8ge de Digne depuis 1806.\"\n)\nfor ent in doc.ents:\n    print(ent, ent.label_)\n\n# 1815 DATE\n# Charles-Fran\u00e7ois-Bienvenu NOM\n# Myriel PRENOM\n# Digne VILLE\n# 1806 DATE\n</code></pre> <p>You can also add the NER component to an existing model (this is only compatible with edsnlp, not spaCy)</p> <pre><code># Given an existing model\nexisting_nlp = ...\n\nexisting_nlp.add_pipe(nlp.get_pipe(\"ner\"), name=\"ner\")\n</code></pre>"}]}